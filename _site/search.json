[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Meet Your Guide to Mastering Accounts and Finance",
    "section": "",
    "text": "Are you ready to dive into the dynamic world of Accounts and Finance?\nWhether you’re a student, professional, or simply curious about the intricacies of financial management, you’ve come to the right place! I’m Dr. Mokaya Nicodemus Oriku, a dedicated lecturer with over 23 of experience in teaching, researching, and consulting in the fields of accounting and finance. My passion lies in demystifying complex financial concepts and empowering my students with practical, real-world insights that go beyond the textbooks.\n\nThrough this blog, I aim to create a vibrant learning space where we explore everything from the fundamentals of accounting to advanced financial strategies. Expect to find engaging articles, helpful tips, and thought-provoking discussions that will not only enhance your knowledge but also inspire you to apply these concepts in your professional journey.\n\nJoin me as we break down the barriers of financial jargon, tackle real-world case studies, and explore the latest trends in the ever-evolving landscape of finance. Let’s turn your financial challenges into opportunities for growth and success!\nStay curious, stay ambitious, and let’s make finance your strength!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Dr. Nicodemus Mokaya",
    "section": "",
    "text": "Hi! My name is Dr. Mokaya Oriku Nicodemus, and I am Seasoned Accounts and Finance Lecturer from Kenya 💜.\nI am mature with high degree of integrity and moral standards. I have excellent ability to guide, counsel and mentor students. I enjoy studying to keep abreast with the changes taking place in the global village.\nI have over 28 years of experience teaching across different universities and colleges on the disciplines of accounting, Research and Finance.\nI hold a PhD in Finance and Account from The prestigious University of Nairobi. My goal is to disseminate knowledge to the entire world and make it a better place for all of us."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Business intelligence tools",
    "section": "",
    "text": "Business Intelligence (BI) tools play a crucial role in turning raw data into actionable insights, aiding decision-makers in making informed choices. In this blog post, we’ll explore the similarities, differences, and unique features of six popular BI tools: Excel, Power BI, Tableau, SAS, Python Dash, and R Shiny. Our analysis will focus on the learning curve and business capability rating of each tool.\n\n\n\nExcel:\n\nSimilarities: Ubiquitous in business for data analysis.\nDifferences: Limited for extensive data processing and visualization.\nUniqueness: Familiar interface but may require advanced functions for complex analytics.\n\nPower BI:\n\nSimilarities: Integrated with Microsoft products.\nDifferences: Emphasis on visualization and dashboards.\nUniqueness: User-friendly, with some learning required for advanced features.\n\nTableau:\n\nSimilarities: Focus on data visualization.\nDifferences: Steeper learning curve; powerful for interactive dashboards.\nUniqueness: Robust visualization capabilities, strong community support.\n\nSAS:\n\nSimilarities: Advanced analytics, statistical analysis.\nDifferences: Requires programming skills; traditional use for complex models.\nUniqueness: Industry-wide usage in healthcare and finance, extensive analytics capabilities.\n\nPython Dash:\n\nSimilarities: Python-based for web-based dashboards.\nDifferences: Programming-centric; suitable for data scientists.\nUniqueness: Flexibility and customization using Python.\n\nR Shiny:\n\nSimilarities: R-based, excellent for statistical analysis.\nDifferences: Requires knowledge of R programming.\nUniqueness: Strong statistical capabilities, ideal for creating interactive web applications.\n\n\n\n\n\n\n\nExcel:\n\nStrengths: Versatile for small to medium-sized datasets.\nWeaknesses: Limited scalability, less advanced analytics.\n\nPower BI:\n\nStrengths: Seamless Microsoft integration, excellent visualizations.\nWeaknesses: May require additional tools for advanced analytics.\n\nTableau:\n\nStrengths: Powerful visualization, extensive data connectivity.\nWeaknesses: Steeper learning curve, higher cost.\n\nSAS:\n\nStrengths: Robust analytics, statistical modeling, and data management.\nWeaknesses: High cost, steeper learning curve.\n\nPython Dash:\n\nStrengths: Customizable with Python, suitable for data science applications.\nWeaknesses: Learning curve for those unfamiliar with Python.\n\nR Shiny:\n\nStrengths: Strong statistical capabilities, great for R users.\nWeaknesses: Learning curve for those unfamiliar with R.\n\n\nIn conclusion, the choice of BI tool depends on specific business needs, data scale, customization requirements, and the existing skill set. Whether opting for user-friendly interfaces or diving into more complex analytics, understanding these tools’ nuances is crucial for effective decision-making."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/welcome/index.html#introduction-to-geospatial-data",
    "href": "posts/welcome/index.html#introduction-to-geospatial-data",
    "title": "Geospatial analysis with R: Lesson1",
    "section": "Introduction to Geospatial Data",
    "text": "Introduction to Geospatial Data\nGeospatial data comes in various forms, such as points, lines, and polygons, each representing different aspects of the Earth’s surface. The most common file formats for storing geospatial data are GeoJSON and Shapefiles.\n\n\n\nGeospatial using R\n\n\n\nLoading Geospatial Data in R\nIn R, the sf package is widely used for handling geospatial data. Let’s start by loading a Shapefile containing information about city boundaries.\n# Install and load required packages\ninstall.packages(\"sf\")\nlibrary(sf)\n\n# Load geospatial data\ncities &lt;- st_read(\"path/to/cities.shp\")\nThis code snippet assumes you have a Shapefile named cities.shp in your working directory. The st_read function from the sf package is used to read the Shapefile and create a spatial data frame.\n\n\nExploring Geospatial Data\nOnce the data is loaded, let’s explore its structure and attributes.\n# Display summary of the spatial data\nsummary(cities)\nThis will provide an overview of the spatial data, including the geometry type (point, line, or polygon), bounding box, and attribute data."
  },
  {
    "objectID": "posts/welcome/index.html#geospatial-data-visualization",
    "href": "posts/welcome/index.html#geospatial-data-visualization",
    "title": "Geospatial analysis with R: Lesson1",
    "section": "Geospatial Data Visualization",
    "text": "Geospatial Data Visualization\nVisualization is crucial for understanding geospatial patterns. We’ll use the ggplot2 package for creating basic maps.\n# Install and load ggplot2\ninstall.packages(\"ggplot2\")\nlibrary(ggplot2)\n\n# Plotting the cities on a map\nggplot() +\n  geom_sf(data = cities) +\n  ggtitle(\"Cities Map\")\nHere, geom_sf is used to plot the spatial features on a map. Customize the plot further by adding layers, adjusting colors, and incorporating additional geospatial data."
  },
  {
    "objectID": "posts/welcome/index.html#spatial-queries-and-analysis",
    "href": "posts/welcome/index.html#spatial-queries-and-analysis",
    "title": "Geospatial analysis with R: Lesson1",
    "section": "Spatial Queries and Analysis",
    "text": "Spatial Queries and Analysis\nPerforming spatial queries allows us to extract meaningful information from geospatial data. Let’s say we want to find cities within a specific region.\n# Define a bounding box for the region\nbbox &lt;- st_bbox(c(xmin, ymin, xmax, ymax), crs = st_crs(cities))\n\n# Extract cities within the bounding box\ncities_in_region &lt;- cities[st_within(cities, st_as_sfc(bbox)), ]\nHere, st_within is used to filter cities that fall within the specified bounding box."
  },
  {
    "objectID": "posts/welcome/index.html#basic-geospatial-statistics",
    "href": "posts/welcome/index.html#basic-geospatial-statistics",
    "title": "Geospatial analysis with R: Lesson1",
    "section": "Basic Geospatial Statistics",
    "text": "Basic Geospatial Statistics\nUnderstanding the spatial distribution of data is crucial. Let’s explore basic spatial statistics using the spdep package.\n# Install and load spdep\ninstall.packages(\"spdep\")\nlibrary(spdep)\n\n# Spatial autocorrelation analysis\nmoran &lt;- moran.test(cities$population, listw = poly2nb(st_as_sfc(cities)))\nprint(moran)\nThis example conducts a Moran’s I test to assess spatial autocorrelation in the population data."
  },
  {
    "objectID": "posts/welcome/index.html#conclusion",
    "href": "posts/welcome/index.html#conclusion",
    "title": "Geospatial analysis with R: Lesson1",
    "section": "Conclusion",
    "text": "Conclusion\nThis blog post has provided a foundational understanding of geospatial data in R. We covered loading data, visualization, spatial queries, and basic statistics. As you delve deeper into geospatial analysis, you’ll find R to be a versatile and powerful tool for unlocking valuable insights from spatial datasets.\nIn the next parts of this series, we will explore advanced topics such as spatial regression, machine learning with geospatial data, and building interactive web maps. Stay tuned for more insights into the fascinating world of spatial analytics with R!"
  },
  {
    "objectID": "posts/post-with-code/index.html#learning-curve",
    "href": "posts/post-with-code/index.html#learning-curve",
    "title": "Business intelligence tools",
    "section": "",
    "text": "Excel:\n\nSimilarities: Ubiquitous in business for data analysis.\nDifferences: Limited for extensive data processing and visualization.\nUniqueness: Familiar interface but may require advanced functions for complex analytics.\n\nPower BI:\n\nSimilarities: Integrated with Microsoft products.\nDifferences: Emphasis on visualization and dashboards.\nUniqueness: User-friendly, with some learning required for advanced features.\n\nTableau:\n\nSimilarities: Focus on data visualization.\nDifferences: Steeper learning curve; powerful for interactive dashboards.\nUniqueness: Robust visualization capabilities, strong community support.\n\nSAS:\n\nSimilarities: Advanced analytics, statistical analysis.\nDifferences: Requires programming skills; traditional use for complex models.\nUniqueness: Industry-wide usage in healthcare and finance, extensive analytics capabilities.\n\nPython Dash:\n\nSimilarities: Python-based for web-based dashboards.\nDifferences: Programming-centric; suitable for data scientists.\nUniqueness: Flexibility and customization using Python.\n\nR Shiny:\n\nSimilarities: R-based, excellent for statistical analysis.\nDifferences: Requires knowledge of R programming.\nUniqueness: Strong statistical capabilities, ideal for creating interactive web applications."
  },
  {
    "objectID": "posts/post-with-code/index.html#business-capability-rating",
    "href": "posts/post-with-code/index.html#business-capability-rating",
    "title": "Business intelligence tools",
    "section": "",
    "text": "Excel:\n\nStrengths: Versatile for small to medium-sized datasets.\nWeaknesses: Limited scalability, less advanced analytics.\n\nPower BI:\n\nStrengths: Seamless Microsoft integration, excellent visualizations.\nWeaknesses: May require additional tools for advanced analytics.\n\nTableau:\n\nStrengths: Powerful visualization, extensive data connectivity.\nWeaknesses: Steeper learning curve, higher cost.\n\nSAS:\n\nStrengths: Robust analytics, statistical modeling, and data management.\nWeaknesses: High cost, steeper learning curve.\n\nPython Dash:\n\nStrengths: Customizable with Python, suitable for data science applications.\nWeaknesses: Learning curve for those unfamiliar with Python.\n\nR Shiny:\n\nStrengths: Strong statistical capabilities, great for R users.\nWeaknesses: Learning curve for those unfamiliar with R.\n\n\nIn conclusion, the choice of BI tool depends on specific business needs, data scale, customization requirements, and the existing skill set. Whether opting for user-friendly interfaces or diving into more complex analytics, understanding these tools’ nuances is crucial for effective decision-making."
  },
  {
    "objectID": "posts/GIS2/index.html",
    "href": "posts/GIS2/index.html",
    "title": "Geospatial Data Manipulation in R: Lesson2",
    "section": "",
    "text": "Geospatial data manipulation is a crucial step in any spatial analysis project. In this blog post, we will delve into the world of geospatial data manipulation using R. Specifically, we’ll explore loading and cleaning free internal data to pave the way for insightful analyses. Let’s embark on this journey together, using a hypothetical scenario where we have access to free internal geospatial data related to public parks.\n\n\nAssuming you have a Shapefile named parks.shp containing information about public parks, we’ll use the sf package to load the data.\n# Install and load required packages\ninstall.packages(\"sf\")\nlibrary(sf)\n\n# Load internal geospatial data (parks)\nparks &lt;- st_read(\"path/to/parks.shp\")\nMake sure to replace “path/to/parks.shp” with the actual path to your Shapefile. The st_read function reads the Shapefile and creates a spatial data frame.\n\n\n\nBefore diving into manipulation, let’s explore the structure and attributes of the loaded geospatial data.\n# Display summary of the spatial data\nsummary(parks)\nThis summary will provide information about the geometry type, bounding box, and attributes of the parks dataset.\n\n\n\nClean data is essential for meaningful analyses. Let’s perform some basic cleaning steps on our parks dataset.\n\n\n# Remove duplicate geometries\nparks &lt;- st_unique(parks)\nThis step ensures that each park is represented only once in the dataset.\n\n\n\n# Check for missing values\nmissing_values &lt;- colSums(is.na(parks))\n\n# Display columns with missing values\nprint(names(missing_values[missing_values &gt; 0]))\nIdentify and handle missing values in relevant columns to maintain data integrity.\n\n\n\n\nVisualizing the geospatial data is a vital step in understanding its characteristics.\n# Plotting parks on a map\nplot(parks, main = \"Public Parks Map\", col = \"green\")\nThis basic map provides a visual overview of the public parks in the dataset. Customize it further using ggplot2 or other plotting libraries for more sophisticated visualizations.\n\n\n\nLet’s perform a basic analysis by calculating the area of each park.\n# Calculate area of parks\nparks$area &lt;- st_area(parks)\nNow, the area column contains the calculated area for each park. This information could be used for further analysis or visualization.\n\n\n\nIn this blog post, we’ve covered the essential steps of loading and cleaning geospatial data using R. Starting with the hypothetical scenario of public parks, we explored data loading, cleaning, and basic analysis. Clean and well-organized geospatial data sets the stage for more advanced spatial analytics and insights.\nIn future articles, we will delve into advanced geospatial analysis, including spatial regression, machine learning with geospatial data, and the creation of interactive web maps. Stay tuned for more exciting explorations into the world of geospatial data science!"
  },
  {
    "objectID": "posts/GIS2/index.html#loading-free-internal-geospatial-data",
    "href": "posts/GIS2/index.html#loading-free-internal-geospatial-data",
    "title": "Geospatial Data Manipulation in R: Lesson2",
    "section": "",
    "text": "Assuming you have a Shapefile named parks.shp containing information about public parks, we’ll use the sf package to load the data.\n# Install and load required packages\ninstall.packages(\"sf\")\nlibrary(sf)\n\n# Load internal geospatial data (parks)\nparks &lt;- st_read(\"path/to/parks.shp\")\nMake sure to replace “path/to/parks.shp” with the actual path to your Shapefile. The st_read function reads the Shapefile and creates a spatial data frame."
  },
  {
    "objectID": "posts/GIS2/index.html#exploring-internal-geospatial-data",
    "href": "posts/GIS2/index.html#exploring-internal-geospatial-data",
    "title": "Geospatial Data Manipulation in R: Lesson2",
    "section": "",
    "text": "Before diving into manipulation, let’s explore the structure and attributes of the loaded geospatial data.\n# Display summary of the spatial data\nsummary(parks)\nThis summary will provide information about the geometry type, bounding box, and attributes of the parks dataset."
  },
  {
    "objectID": "posts/GIS2/index.html#cleaning-geospatial-data",
    "href": "posts/GIS2/index.html#cleaning-geospatial-data",
    "title": "Geospatial Data Manipulation in R: Lesson2",
    "section": "",
    "text": "Clean data is essential for meaningful analyses. Let’s perform some basic cleaning steps on our parks dataset.\n\n\n# Remove duplicate geometries\nparks &lt;- st_unique(parks)\nThis step ensures that each park is represented only once in the dataset.\n\n\n\n# Check for missing values\nmissing_values &lt;- colSums(is.na(parks))\n\n# Display columns with missing values\nprint(names(missing_values[missing_values &gt; 0]))\nIdentify and handle missing values in relevant columns to maintain data integrity."
  },
  {
    "objectID": "posts/GIS2/index.html#spatial-data-exploration",
    "href": "posts/GIS2/index.html#spatial-data-exploration",
    "title": "Geospatial Data Manipulation in R: Lesson2",
    "section": "",
    "text": "Visualizing the geospatial data is a vital step in understanding its characteristics.\n# Plotting parks on a map\nplot(parks, main = \"Public Parks Map\", col = \"green\")\nThis basic map provides a visual overview of the public parks in the dataset. Customize it further using ggplot2 or other plotting libraries for more sophisticated visualizations."
  },
  {
    "objectID": "posts/GIS2/index.html#basic-geospatial-analysis",
    "href": "posts/GIS2/index.html#basic-geospatial-analysis",
    "title": "Geospatial Data Manipulation in R: Lesson2",
    "section": "",
    "text": "Let’s perform a basic analysis by calculating the area of each park.\n# Calculate area of parks\nparks$area &lt;- st_area(parks)\nNow, the area column contains the calculated area for each park. This information could be used for further analysis or visualization."
  },
  {
    "objectID": "posts/GIS2/index.html#conclusion",
    "href": "posts/GIS2/index.html#conclusion",
    "title": "Geospatial Data Manipulation in R: Lesson2",
    "section": "",
    "text": "In this blog post, we’ve covered the essential steps of loading and cleaning geospatial data using R. Starting with the hypothetical scenario of public parks, we explored data loading, cleaning, and basic analysis. Clean and well-organized geospatial data sets the stage for more advanced spatial analytics and insights.\nIn future articles, we will delve into advanced geospatial analysis, including spatial regression, machine learning with geospatial data, and the creation of interactive web maps. Stay tuned for more exciting explorations into the world of geospatial data science!"
  },
  {
    "objectID": "posts/spatial regression/index.html",
    "href": "posts/spatial regression/index.html",
    "title": "Spartial regression: Lesson3",
    "section": "",
    "text": "Spatial regression is a powerful technique in spatial analytics that allows us to model relationships between variables while accounting for the spatial dependencies inherent in geospatial data. In this advanced blog post, we will dive into the intricacies of spatial regression using R. Our goal is to uncover hidden patterns and relationships in geospatial datasets, focusing on a hypothetical scenario of housing prices and neighborhood characteristics."
  },
  {
    "objectID": "posts/spatial regression/index.html#understanding-spatial-regression",
    "href": "posts/spatial regression/index.html#understanding-spatial-regression",
    "title": "Spartial regression: Lesson3",
    "section": "Understanding Spatial Regression",
    "text": "Understanding Spatial Regression\nSpatial regression extends traditional regression models by incorporating spatial relationships. It acknowledges that observations closer in space may exhibit similarities or dependencies that traditional models overlook. There are different types of spatial regression models, and in this blog, we will focus on the Spatial Lag Model.\n\nSpatial Lag Model\nThe Spatial Lag Model introduces a spatially lagged dependent variable, indicating the influence of neighboring observations. Let’s consider housing prices as the dependent variable and neighborhood characteristics as independent variables.\n# Install and load required packages\ninstall.packages(\"spdep\")\nlibrary(spdep)\n\n# Load geospatial data (housing prices and neighborhood characteristics)\nhousing_data &lt;- st_read(\"path/to/housing_data.shp\")\n\n# Create spatial weights matrix\nw &lt;- poly2nb(st_as_sfc(housing_data))\nlw &lt;- nb2listw(w)\n\n# Fit Spatial Lag Model\nmodel &lt;- lm(y ~ x1 + x2 + lag(y, listw = lw), data = housing_data)\nsummary(model)\n\nReplace “path/to/housing_data.shp” with the actual path to your Shapefile. This example assumes you have a dependent variable y (housing prices) and independent variables x1 and x2 (neighborhood characteristics)."
  },
  {
    "objectID": "posts/spatial regression/index.html#interpretation-of-results",
    "href": "posts/spatial regression/index.html#interpretation-of-results",
    "title": "Spartial regression: Lesson3",
    "section": "Interpretation of Results",
    "text": "Interpretation of Results\nThe summary output provides information about coefficients, standard errors, and statistical significance. Pay special attention to the spatial lag coefficient, which indicates the impact of neighboring observations on the dependent variable."
  },
  {
    "objectID": "posts/spatial regression/index.html#diagnostic-checks",
    "href": "posts/spatial regression/index.html#diagnostic-checks",
    "title": "Spartial regression: Lesson3",
    "section": "Diagnostic Checks",
    "text": "Diagnostic Checks\nAssess the model’s validity and assumptions through diagnostic checks.\n# Spatial autocorrelation of residuals\nresiduals &lt;- residuals(model)\nmoran.test(residuals, listw = lw)\n\nA significant Moran’s I statistic for residuals indicates spatial autocorrelation, suggesting the need for further model refinement."
  },
  {
    "objectID": "posts/spatial regression/index.html#visualization",
    "href": "posts/spatial regression/index.html#visualization",
    "title": "Spartial regression: Lesson3",
    "section": "Visualization",
    "text": "Visualization\nVisualize the spatial patterns and regression results on a map.\n# Plotting observed vs. predicted values\nplot(housing_data$y, fitted(model), main = \"Observed vs. Predicted\", xlab = \"Observed\", ylab = \"Predicted\")\n\n# Spatial autocorrelation map of residuals\nspplot(residuals, main = \"Spatial Autocorrelation Map of Residuals\", col.regions = colorRampPalette(c(\"blue\", \"white\", \"red\")))\n\nThese visualizations help in understanding how well the model captures spatial patterns and where adjustments might be needed."
  },
  {
    "objectID": "posts/spatial regression/index.html#conclusion",
    "href": "posts/spatial regression/index.html#conclusion",
    "title": "Spartial regression: Lesson3",
    "section": "Conclusion",
    "text": "Conclusion\nSpatial regression in R opens up new dimensions for analyzing geospatial data. In this blog post, we explored the Spatial Lag Model as an advanced technique for modeling spatial dependencies in housing prices and neighborhood characteristics. The interpretation of results, diagnostic checks, and visualizations are crucial components of spatial regression analysis.\nAs you venture into spatial analytics, consider exploring other spatial regression models, incorporating additional spatial variables, and applying advanced techniques to enhance the robustness of your models. Stay tuned for more advanced spatial analytics topics, including machine learning with geospatial data and building interactive web maps. Happy analyzing!"
  },
  {
    "objectID": "posts/spatial regression/index.html#mastering-spatial-regression-in-r-unveiling-patterns-in-geospatial-data",
    "href": "posts/spatial regression/index.html#mastering-spatial-regression-in-r-unveiling-patterns-in-geospatial-data",
    "title": "Spartial regression: Lesson3",
    "section": "",
    "text": "Spatial regression is a powerful technique in spatial analytics that allows us to model relationships between variables while accounting for the spatial dependencies inherent in geospatial data. In this advanced blog post, we will dive into the intricacies of spatial regression using R. Our goal is to uncover hidden patterns and relationships in geospatial datasets, focusing on a hypothetical scenario of housing prices and neighborhood characteristics."
  },
  {
    "objectID": "posts/How to build a shinyapp/index.html",
    "href": "posts/How to build a shinyapp/index.html",
    "title": "How to build a ShinyApp",
    "section": "",
    "text": "We begin to demonstrate the building blocks of a shinyApp.\nAn App needs a *User interface (ui)* and a server. The majic about the *shiny package* is that it can create both of this within R, plus run your app using an additionational shiny function.\nFirst,\n\nload the library using the shiny.\n\n\nlibrary(shiny)\n\n\nCreate ui using the html function\n\n\nui &lt;- fluidPage()\n\n\nDefine a custom function to create the server\n\n\nserver &lt;- function(input,\n\noutput,\n\nsession){\n\n}\n\n\nfinally run your app.\n\n\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "posts/How to build a shinyapp/index.html#building-a-hello-world-shinyapp",
    "href": "posts/How to build a shinyapp/index.html#building-a-hello-world-shinyapp",
    "title": "How to build a ShinyApp",
    "section": "",
    "text": "We begin to demonstrate the building blocks of a shinyApp.\nAn App needs a *User interface (ui)* and a server. The majic about the *shiny package* is that it can create both of this within R, plus run your app using an additionational shiny function.\nFirst,\n\nload the library using the shiny.\n\n\nlibrary(shiny)\n\n\nCreate ui using the html function\n\n\nui &lt;- fluidPage()\n\n\nDefine a custom function to create the server\n\n\nserver &lt;- function(input,\n\noutput,\n\nsession){\n\n}\n\n\nfinally run your app.\n\n\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "posts/How to build a shinyapp/index.html#example1-of-shiny-app",
    "href": "posts/How to build a shinyapp/index.html#example1-of-shiny-app",
    "title": "How to build a ShinyApp",
    "section": "Example1 of shiny app",
    "text": "Example1 of shiny app\n\nlibrary(shiny)\n\nlibrary(widgetframe)\n\nui &lt;- fluidPage(\n\n\"Hello, world!!!!!!\"\n\n)\n\nserver &lt;- function(input,\n\noutput,\n\nsession){\n\n}\n\nshinyApp(ui = ui, server = server)\n\n\nExample2: Add a question\nWe want to go an extra mile an add a text that asks a question. This is possible but adding *textinput* function that allows us to enter text. It has three arguments, a unique ID that will be used to refer to this input, a label that is displayed to the user and an optional default value.\nOur full out put that is diplayed is contained in the server using the render text function. Inside of that you can use *paste* to create a longer character string. And if add *input$name* you can access the name added using text input. The text is assigned to an output object that will be used in the ui to display.\n\nlibrary(shiny)\n\nlibrary(widgetframe)\n\nui &lt;- fluidPage(\n\ntextInput(\"name\", \"Enter your name:\"),\n\ntextOutput(\"r\")\n\n)\n\nserver &lt;- function(input, output){\n\noutput$r &lt;- renderText({\n\npaste0(\"Do you prefer rain or sunshine,\", input$name, \"?\")\n\n})\n\n}\n\nshinyApp(ui = ui, server = server)\n\nYou did it a text that uses a text input!!"
  },
  {
    "objectID": "posts/getting data from Api/index.html",
    "href": "posts/getting data from Api/index.html",
    "title": "Getting Weather Data via API with R",
    "section": "",
    "text": "I came across this awesome post from Albert Rapp about how to get weather data from API and I thought it was a great technique to use. Here we go.\n\nWhat is an API?\nWe are focusing on two APIs (application programming interfaces) for our project. Broadly speaking, an API is anything that we can throw code at to get results that we want.\nOften this refers to some data source that we tap into. But sometimes it also simply means the syntax of code. For example, ggplot2 has a very distinctive API, i.e. a code syntax to create a chart.\nIn our current case, we will just refer to APIs as data sources and we will need to tap into two such APIs, namely these ones:\n\nUS National Weather Service API\nGoogle Geocoding API\n\nThe first one will give us weather forecasts based on specified coordinates and the second one will turn any address into coordinates for us. Today, we’ll focus on the first one.\n\n\nMaking requests to an API\nIf you’ve never worked with APIs, you know that it can feel like data is hidden away behind an API. Thankfully, the {httr2} package helps us a lot, even if we’ve never dealt with APIs before.\nCase in point, my fellow YouTubeR (see what I did there? it’s “YouTube” and “R”) Melissa Van Bussel put together an excellent video that shows you how to use {httr2} to call the API of openAI or GitLab.\nAnyway, here’s how to make a request to an API to get data:\n\nNavigate to the URL the data can be accessed from\n(Optional depending on the API) Authenticate\nGet the response\n\nWith the National Weather Service, you can easily try this yourself. Just head to the following url using your web browser:\nhttps://api.weather.gov/points/38.8894,-77.0352\nIf you navigate there, you will get cryptic data like that:\n\nThis is what is known as a JSON file. More on that later. For now, notice that what you see at the end of the url after points/ corresponds to the coordinates that are given in the JSON output.\nThis means that the recipe for calling the weather API is simple: Append points/{lat},{long} at the end of the base_url, i.e. https://api.weather.gov/. In this case, {lat},{long} corresponds to the latitude and longitude of the location you want to get weather forecasts for.\n\n\nMaking a request with {httr2}\nThe {httr2} syntax to make this work mimics this quite well. Here’s how it looks.\n\nBasically, at the core of every request is the request() function that needs to know the base_url. This returns an &lt;httr2_request&gt; object that can be passed to further req_*() functions to modify the request.\nHere, we used req_url_path_append() to modify the request but there are also other functions (and next week we’ll learn about req_url_query()). Finally, to actually make the request, you can pass everything to req_perform().\n\n\n\nGetting the response\nAs you’ve just seen, your request will return a &lt;httr2_response&gt; and if everything went well, the output will also show you Status: 200 OK. You can get the actual content (the JSON that you’ve seen in your web browser earlier) via one of the many resp_*() functions that handle responses.\n\n\n\nWorking with the response\nAs you’ve seen in the output, the JSON file you receive is structured as a highly nested list. To make sense of this data, we use glimpse() to understand the structure of the object\n\nAnd with pluck() you can easily drill down into specific parts of the list. For example, this could be used to get the URL for the hourly forecasts\n\n\n\nRepeat process for forecasts\nWith the new forecast URL, we can get new JSON data about the forecasts for our location.\n\nIn that output, we can see that there is a list called periods inside of the properties list that contains lists which always have 16 entries. This might be where our forecast data lives.\n\nAha! Each of those 16-entries lists seem to correspond to the forecast of one hour. Thus, to get the things like temperature, startTime, probabilityOfPrecipitation, and short_forecast into a nice tibble format, we have to iterate over all of these 16-entries lists and wrap the values we want into a tibble.\n\nAnd once we have that data, we can transform the time column into a nice date-time object:\n\nThis was awesome. !"
  },
  {
    "objectID": "posts/3 categorical variables and two numeric vars on one plot/index.html",
    "href": "posts/3 categorical variables and two numeric vars on one plot/index.html",
    "title": "Creating Beautiful Visualizations of Educational Attainment Using ggplot2 and ggbeeswarm",
    "section": "",
    "text": "In the realm of data visualization, crafting compelling and informative graphics is an art. In this blog post, we will explore the step-by-step procedure of creating a visually stunning representation of educational attainment scores across different town sizes and income deprivation groups. Our tool of choice for this journey will be the powerful combination of ggplot2 and ggbeeswarm packages in R.\n\n\n\n\n\n\nBefore diving into the visualization process, we need to set up our R environment and load the necessary packages. Ensure you have ggplot2 and ggbeeswarm installed. Then, import your dataset containing information about educational attainment, town sizes, and income deprivation groups.\nlibrary(tidyverse)\nlibrary(ggbeeswarm)\nlibrary(camcorder)\n\n# Idea and text from\n# https://www.ons.gov.uk/peoplepopulationandcommunity/educationandchildcare/articles/whydochildrenandyoungpeopleinsmallertownsdobetteracademicallythanthoseinlargertowns/2023-07-25\n\n# set the graphic auto save\ngg_record(dir = \"tidytuesday-temp\", device = \"png\", width = 9, height = 8, units = \"in\", dpi = 320)\n\nenglish_education &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-01-23/english_education.csv')\n\n\n\nTake a moment to explore your dataset and understand its structure. Identify the variables of interest and ensure they are in the right format for visualization. Clean the data if needed.\nee &lt;- english_education %&gt;% \n  filter(!size_flag %in% c(\"Not BUA\", \"Other Small BUAs\")) %&gt;% \n  mutate(\n    size = case_when(\n      str_detect(tolower(size_flag), \"london\") ~ \"Inner and outer London\",\n      size_flag == \"City\" ~ \"Cities (excluding London)\",\n      TRUE ~ size_flag\n    ),\n    income = case_when(\n      str_detect(income_flag, \"deprivation\") ~ income_flag,\n      TRUE ~ NA\n    )\n  ) %&gt;% \n  mutate(\n    size = fct_inorder(size), # order the names \n    income = fct_relevel(income, \n                         \"Higher deprivation towns\",\n                         \"Mid deprivation towns\",\n                         \"Lower deprivation towns\",\n    )\n  )\nCreate a summarized median educational score for each London region\nee_size_med &lt;- ee %&gt;% \n  group_by(size) %&gt;% \n  summarise(median = median(education_score, na.rm = TRUE))\n\n\n\nNow, let’s move on to the heart of our blog post - the creation of our beautiful visualization. We will utilize ggplot2 for its flexibility and ggbeeswarm for its ability to handle overlapping points gracefully.\n\nf1 &lt;- \"Outfit\"\npal &lt;- MetBrewer::met.brewer(\"Klimt\", 4)\ncol_purple &lt;- MetBrewer::met.brewer(\"Klimt\")[1]\n\nggplot() +\n  # Median and annotation\n  geom_vline(data = ee_size_med, aes(xintercept = median),\n             color = col_purple, linewidth = 1) +\n  geom_text(data = ee_size_med %&gt;% \n              filter(size == \"Large Towns\"), \n            aes(median,  Inf, label = \"Average for size group\"), hjust = 1,\n            nudge_x = -1.5, family = f1, fontface = \"bold\", \n            size = 3.5, color = col_purple) +\n  geom_curve(data = ee_size_med %&gt;% \n               filter(size == \"Large Towns\"),\n             aes(x = median - 1.2, xend = median - 0.2,\n                 y = Inf, yend = Inf), color = col_purple, \n             curvature = -0.3, arrow = arrow(length = unit(0.1, \"npc\"))) +\n  # Towns\n  geom_quasirandom(data = ee, aes(education_score, size, color = income)\n                   , alpha = 0.5, dodge.width = 2, method = \"pseudorandom\", size = 2.5) +\n  scale_color_manual(values = pal, na.value = \"grey20\", \n                     breaks = c(\"Lower deprivation towns\", \"Mid deprivation towns\", \"Higher deprivation towns\"), \n                     labels = c(\"Lower\", \"Mid\", \"Higher\")) +\n  scale_x_continuous(minor_break = (-10:10)) +\n  coord_cartesian(clip = \"off\") +\n  facet_wrap(vars(size), ncol = 1, scales = \"free_y\") +\n  labs(\n    title = \"Smaller towns have the highest average educational attainment\",\n    subtitle = \"Educational attainment score, by town size and income deprivation group\",\n    caption = \"Source: UK Office for National Statistics · Graphic: Georgios Karamanis\",\n    x = paste0(\"← Lower attainment\", strrep(\" \", 30), \"Educational attainment index score\", \n               strrep(\" \", 30), \"Higher attainment →\"),\n    color = \"Town income\\ndeprivation\"\n  ) +\n  theme_minimal(base_family = f1) +\n  theme(\n    plot.background = element_rect(fill = \"grey99\", color = NA),\n    legend.position = c(0.88, 0.3),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.title.x = element_text(hjust = 0.32),\n    plot.margin = margin(10, 10, 10, 10),\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.caption = element_text(margin = margin(10, 0, 0, 0)),\n    panel.spacing.y = unit(1, \"lines\"),\n    strip.text = element_text(size = 11, face = \"bold\", margin = margin(10, 0, 10, 0))\n  )\nThis code creates a beeswarm plot, where each point represents a data point, and the median is visualized as a crossbar. This is a beautiful viz of 3 numeric variables, one numeric variable and a median as a measure of central tendancies. The color aesthetic distinguishes income deprivation groups, providing a comprehensive view of the educational attainment landscape.\n\n\n\nBy following this step-by-step guide, you’ve learned how to use ggplot2 and ggbeeswarm to create an aesthetically pleasing and informative visualization of educational attainment scores. This not only enhances your storytelling capabilities but also adds a touch of elegance to your data presentations.\nRemember, the art of visualization lies not just in the final result but in the thoughtful process that leads to it. Now, armed with this knowledge, go forth and create compelling visuals that resonate with your audience."
  },
  {
    "objectID": "posts/3 categorical variables and two numeric vars on one plot/index.html#introduction",
    "href": "posts/3 categorical variables and two numeric vars on one plot/index.html#introduction",
    "title": "Creating Beautiful Visualizations of Educational Attainment Using ggplot2 and ggbeeswarm",
    "section": "",
    "text": "In the realm of data visualization, crafting compelling and informative graphics is an art. In this blog post, we will explore the step-by-step procedure of creating a visually stunning representation of educational attainment scores across different town sizes and income deprivation groups. Our tool of choice for this journey will be the powerful combination of ggplot2 and ggbeeswarm packages in R."
  },
  {
    "objectID": "posts/3 categorical variables and two numeric vars on one plot/index.html#step-1-setup-and-data-loading",
    "href": "posts/3 categorical variables and two numeric vars on one plot/index.html#step-1-setup-and-data-loading",
    "title": "Creating Beautiful Visualizations of Educational Attainment Using ggplot2 and ggbeeswarm",
    "section": "",
    "text": "Before diving into the visualization process, we need to set up our R environment and load the necessary packages. Ensure you have ggplot2 and ggbeeswarm installed. Then, import your dataset containing information about educational attainment, town sizes, and income deprivation groups.\nlibrary(tidyverse)\nlibrary(ggbeeswarm)\nlibrary(camcorder)\n\n# Idea and text from\n# https://www.ons.gov.uk/peoplepopulationandcommunity/educationandchildcare/articles/whydochildrenandyoungpeopleinsmallertownsdobetteracademicallythanthoseinlargertowns/2023-07-25\n\n# set the graphic auto save\ngg_record(dir = \"tidytuesday-temp\", device = \"png\", width = 9, height = 8, units = \"in\", dpi = 320)\n\nenglish_education &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-01-23/english_education.csv')"
  },
  {
    "objectID": "posts/3 categorical variables and two numeric vars on one plot/index.html#step-2-data-exploration-and-preparation",
    "href": "posts/3 categorical variables and two numeric vars on one plot/index.html#step-2-data-exploration-and-preparation",
    "title": "Creating Beautiful Visualizations of Educational Attainment Using ggplot2 and ggbeeswarm",
    "section": "",
    "text": "Take a moment to explore your dataset and understand its structure. Identify the variables of interest and ensure they are in the right format for visualization. Clean the data if needed.\nee &lt;- english_education %&gt;% \n  filter(!size_flag %in% c(\"Not BUA\", \"Other Small BUAs\")) %&gt;% \n  mutate(\n    size = case_when(\n      str_detect(tolower(size_flag), \"london\") ~ \"Inner and outer London\",\n      size_flag == \"City\" ~ \"Cities (excluding London)\",\n      TRUE ~ size_flag\n    ),\n    income = case_when(\n      str_detect(income_flag, \"deprivation\") ~ income_flag,\n      TRUE ~ NA\n    )\n  ) %&gt;% \n  mutate(\n    size = fct_inorder(size), # order the names \n    income = fct_relevel(income, \n                         \"Higher deprivation towns\",\n                         \"Mid deprivation towns\",\n                         \"Lower deprivation towns\",\n    )\n  )\nCreate a summarized median educational score for each London region\nee_size_med &lt;- ee %&gt;% \n  group_by(size) %&gt;% \n  summarise(median = median(education_score, na.rm = TRUE))"
  },
  {
    "objectID": "posts/3 categorical variables and two numeric vars on one plot/index.html#step-3-creating-the-visualization",
    "href": "posts/3 categorical variables and two numeric vars on one plot/index.html#step-3-creating-the-visualization",
    "title": "Creating Beautiful Visualizations of Educational Attainment Using ggplot2 and ggbeeswarm",
    "section": "",
    "text": "Now, let’s move on to the heart of our blog post - the creation of our beautiful visualization. We will utilize ggplot2 for its flexibility and ggbeeswarm for its ability to handle overlapping points gracefully.\n\nf1 &lt;- \"Outfit\"\npal &lt;- MetBrewer::met.brewer(\"Klimt\", 4)\ncol_purple &lt;- MetBrewer::met.brewer(\"Klimt\")[1]\n\nggplot() +\n  # Median and annotation\n  geom_vline(data = ee_size_med, aes(xintercept = median),\n             color = col_purple, linewidth = 1) +\n  geom_text(data = ee_size_med %&gt;% \n              filter(size == \"Large Towns\"), \n            aes(median,  Inf, label = \"Average for size group\"), hjust = 1,\n            nudge_x = -1.5, family = f1, fontface = \"bold\", \n            size = 3.5, color = col_purple) +\n  geom_curve(data = ee_size_med %&gt;% \n               filter(size == \"Large Towns\"),\n             aes(x = median - 1.2, xend = median - 0.2,\n                 y = Inf, yend = Inf), color = col_purple, \n             curvature = -0.3, arrow = arrow(length = unit(0.1, \"npc\"))) +\n  # Towns\n  geom_quasirandom(data = ee, aes(education_score, size, color = income)\n                   , alpha = 0.5, dodge.width = 2, method = \"pseudorandom\", size = 2.5) +\n  scale_color_manual(values = pal, na.value = \"grey20\", \n                     breaks = c(\"Lower deprivation towns\", \"Mid deprivation towns\", \"Higher deprivation towns\"), \n                     labels = c(\"Lower\", \"Mid\", \"Higher\")) +\n  scale_x_continuous(minor_break = (-10:10)) +\n  coord_cartesian(clip = \"off\") +\n  facet_wrap(vars(size), ncol = 1, scales = \"free_y\") +\n  labs(\n    title = \"Smaller towns have the highest average educational attainment\",\n    subtitle = \"Educational attainment score, by town size and income deprivation group\",\n    caption = \"Source: UK Office for National Statistics · Graphic: Georgios Karamanis\",\n    x = paste0(\"← Lower attainment\", strrep(\" \", 30), \"Educational attainment index score\", \n               strrep(\" \", 30), \"Higher attainment →\"),\n    color = \"Town income\\ndeprivation\"\n  ) +\n  theme_minimal(base_family = f1) +\n  theme(\n    plot.background = element_rect(fill = \"grey99\", color = NA),\n    legend.position = c(0.88, 0.3),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.title.x = element_text(hjust = 0.32),\n    plot.margin = margin(10, 10, 10, 10),\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.caption = element_text(margin = margin(10, 0, 0, 0)),\n    panel.spacing.y = unit(1, \"lines\"),\n    strip.text = element_text(size = 11, face = \"bold\", margin = margin(10, 0, 10, 0))\n  )\nThis code creates a beeswarm plot, where each point represents a data point, and the median is visualized as a crossbar. This is a beautiful viz of 3 numeric variables, one numeric variable and a median as a measure of central tendancies. The color aesthetic distinguishes income deprivation groups, providing a comprehensive view of the educational attainment landscape."
  },
  {
    "objectID": "posts/3 categorical variables and two numeric vars on one plot/index.html#conclusion",
    "href": "posts/3 categorical variables and two numeric vars on one plot/index.html#conclusion",
    "title": "Creating Beautiful Visualizations of Educational Attainment Using ggplot2 and ggbeeswarm",
    "section": "",
    "text": "By following this step-by-step guide, you’ve learned how to use ggplot2 and ggbeeswarm to create an aesthetically pleasing and informative visualization of educational attainment scores. This not only enhances your storytelling capabilities but also adds a touch of elegance to your data presentations.\nRemember, the art of visualization lies not just in the final result but in the thoughtful process that leads to it. Now, armed with this knowledge, go forth and create compelling visuals that resonate with your audience."
  },
  {
    "objectID": "posts/Mathematical and analytical skills/index.html",
    "href": "posts/Mathematical and analytical skills/index.html",
    "title": "Crunching Numbers with a Smile: A Playful Guide to Elevating Mathematical and Analytical Skills in the Workplace",
    "section": "",
    "text": "Ahoy, fellow math maestros and numerical neophytes alike! In the vast terrain of the professional realm, where equations reign supreme and spreadsheets take center stage, it’s time to sprinkle a dash of humor onto the canvas of calculations. Join me on this delightful journey as we explore ways to enhance our mathematical and analytical prowess with a grin on our faces.\n\n\n\n\n\n\n\n\nEmbrace mathematics as the quirky companion in your cubicle. Inject humor into the daily grind of numbers. Consider math the office prankster – sometimes challenging, often surprising, but always ready to bring a smile to your face. Crack a math-related joke during coffee breaks and transform your numerical nemesis into a trusted ally.\nExample: “Why was the equal sign so humble? Because he knew he wasn't less than or greater than anyone else!”\n\n\n\n\n\n\n\n\nExcel, the caped crusader of corporate calculation! Dive into the realm of formulas and functions fearlessly, like a hero facing down a villain. Master PivotTables, VLOOKUPs, and SUMIFS – wield them with the finesse of a mathematical ninja. Soon, your coworkers will marvel at your spreadsheet superpowers, making you the hero the office never knew it needed.\n\n\n\n\n\n\n\n\nIntroduce brain-teasing puzzles and games into your daily routine – because who said work can’t be fun? Sudoku, crosswords, and logic puzzles not only entertain but also supercharge your analytical thinking. Challenge your colleagues to friendly puzzle-solving competitions during breaks. The winner earns bragging rights and a metaphorical crown, turning the office into a playground of mental gymnastics.\n\n\n\nTransform mundane meetings into engaging sessions by incorporating brain teasers and quick math challenges. Kickstart discussions with a burst of intellectual energy. Not only will this sharpen everyone’s analytical skills, but it will also foster a collaborative and dynamic workplace.\nExample: Open your next meeting with a puzzle: “If it takes 5 machines 5 minutes to make 5 widgets, how long would it take 100 machines to make 100 widgets?” Watch as your colleagues’ brains kick into high gear!\n\n\n\nCharts and graphs need not be dull – let them be your canvas for creativity! Infuse a bit of humor into your data visualizations. Turn a line graph into a rollercoaster of success or transform a bar chart into a skyscraper of achievements. The more entertaining, the more memorable your insights become.\n\n\n\nIn the pursuit of mathematical and analytical excellence, don’t forget to have a bit of fun along the way. Whether you’re a seasoned number-cruncher or a math-phobic beginner, infusing humor into your calculations not only enhances your skills but also makes the workplace a more enjoyable space for everyone. So, put on your mathematical cape, embrace the numbers, and let the workplace be your stage for a comedy of calculations. Happy calculating!"
  },
  {
    "objectID": "posts/Mathematical and analytical skills/index.html#introduction",
    "href": "posts/Mathematical and analytical skills/index.html#introduction",
    "title": "Crunching Numbers with a Smile: A Playful Guide to Elevating Mathematical and Analytical Skills in the Workplace",
    "section": "",
    "text": "Ahoy, fellow math maestros and numerical neophytes alike! In the vast terrain of the professional realm, where equations reign supreme and spreadsheets take center stage, it’s time to sprinkle a dash of humor onto the canvas of calculations. Join me on this delightful journey as we explore ways to enhance our mathematical and analytical prowess with a grin on our faces."
  },
  {
    "objectID": "posts/Mathematical and analytical skills/index.html#counting-chuckles-a-positive-perspective",
    "href": "posts/Mathematical and analytical skills/index.html#counting-chuckles-a-positive-perspective",
    "title": "Crunching Numbers with a Smile: A Playful Guide to Elevating Mathematical and Analytical Skills in the Workplace",
    "section": "",
    "text": "Embrace mathematics as the quirky companion in your cubicle. Inject humor into the daily grind of numbers. Consider math the office prankster – sometimes challenging, often surprising, but always ready to bring a smile to your face. Crack a math-related joke during coffee breaks and transform your numerical nemesis into a trusted ally.\nExample: “Why was the equal sign so humble? Because he knew he wasn't less than or greater than anyone else!”"
  },
  {
    "objectID": "posts/Mathematical and analytical skills/index.html#excel-erate-your-skills-unleashing-spreadsheet-superpowers",
    "href": "posts/Mathematical and analytical skills/index.html#excel-erate-your-skills-unleashing-spreadsheet-superpowers",
    "title": "Crunching Numbers with a Smile: A Playful Guide to Elevating Mathematical and Analytical Skills in the Workplace",
    "section": "",
    "text": "Excel, the caped crusader of corporate calculation! Dive into the realm of formulas and functions fearlessly, like a hero facing down a villain. Master PivotTables, VLOOKUPs, and SUMIFS – wield them with the finesse of a mathematical ninja. Soon, your coworkers will marvel at your spreadsheet superpowers, making you the hero the office never knew it needed."
  },
  {
    "objectID": "posts/Mathematical and analytical skills/index.html#the-power-of-puzzles-mental-gymnastics-for-all",
    "href": "posts/Mathematical and analytical skills/index.html#the-power-of-puzzles-mental-gymnastics-for-all",
    "title": "Crunching Numbers with a Smile: A Playful Guide to Elevating Mathematical and Analytical Skills in the Workplace",
    "section": "",
    "text": "Introduce brain-teasing puzzles and games into your daily routine – because who said work can’t be fun? Sudoku, crosswords, and logic puzzles not only entertain but also supercharge your analytical thinking. Challenge your colleagues to friendly puzzle-solving competitions during breaks. The winner earns bragging rights and a metaphorical crown, turning the office into a playground of mental gymnastics."
  },
  {
    "objectID": "posts/Mathematical and analytical skills/index.html#math-meetings-with-a-twist-brainy-banter-for-team-bonding",
    "href": "posts/Mathematical and analytical skills/index.html#math-meetings-with-a-twist-brainy-banter-for-team-bonding",
    "title": "Crunching Numbers with a Smile: A Playful Guide to Elevating Mathematical and Analytical Skills in the Workplace",
    "section": "",
    "text": "Transform mundane meetings into engaging sessions by incorporating brain teasers and quick math challenges. Kickstart discussions with a burst of intellectual energy. Not only will this sharpen everyone’s analytical skills, but it will also foster a collaborative and dynamic workplace.\nExample: Open your next meeting with a puzzle: “If it takes 5 machines 5 minutes to make 5 widgets, how long would it take 100 machines to make 100 widgets?” Watch as your colleagues’ brains kick into high gear!"
  },
  {
    "objectID": "posts/Mathematical and analytical skills/index.html#graphs-with-a-grin-visualizing-data-creatively",
    "href": "posts/Mathematical and analytical skills/index.html#graphs-with-a-grin-visualizing-data-creatively",
    "title": "Crunching Numbers with a Smile: A Playful Guide to Elevating Mathematical and Analytical Skills in the Workplace",
    "section": "",
    "text": "Charts and graphs need not be dull – let them be your canvas for creativity! Infuse a bit of humor into your data visualizations. Turn a line graph into a rollercoaster of success or transform a bar chart into a skyscraper of achievements. The more entertaining, the more memorable your insights become."
  },
  {
    "objectID": "posts/Mathematical and analytical skills/index.html#conclusion",
    "href": "posts/Mathematical and analytical skills/index.html#conclusion",
    "title": "Crunching Numbers with a Smile: A Playful Guide to Elevating Mathematical and Analytical Skills in the Workplace",
    "section": "",
    "text": "In the pursuit of mathematical and analytical excellence, don’t forget to have a bit of fun along the way. Whether you’re a seasoned number-cruncher or a math-phobic beginner, infusing humor into your calculations not only enhances your skills but also makes the workplace a more enjoyable space for everyone. So, put on your mathematical cape, embrace the numbers, and let the workplace be your stage for a comedy of calculations. Happy calculating!"
  },
  {
    "objectID": "posts/Project management/index.html",
    "href": "posts/Project management/index.html",
    "title": "Mastering Project Management: A Simple Guide to Success",
    "section": "",
    "text": "Mastering Project Management: A Simple Guide to Success\n\nIntroduction:\nHello, project enthusiasts! Whether you’re a seasoned project manager or someone stepping into the world of coordinating tasks, we’re here to make project management a breeze. In this easy-to-understand guide, we’ll explore how to boost your competence, ensure timely delivery, maintain high-quality output, and foster great communication. Let’s break it down without the jargon and make project success accessible to all!\n1. “Competence: The ABCs of Mastery”\n\n\n\n\n\nImagine project management as a skill playground. Start by learning the basics – understand your project’s objectives, scope, and team dynamics. Familiarize yourself with project management tools; they’re like the cool kids’ toys that make your job easier. As you play in this skill playground, you’ll naturally enhance your competence over time.\n2. “Timely Delivery: The Art of Time Management”\n\n\n\n\n\nThink of project timelines as a recipe – break down the tasks into bite-sized pieces. Create a timeline that’s realistic and achievable. Use a calendar or a project management tool to keep everyone on the same page. Remember, punctuality is the secret ingredient to successful project delivery.\n3. “Quality Output: Crafting a Masterpiece”\n\n\n\n\n\nQuality output is like baking a cake – you need the right ingredients and a foolproof recipe. Clearly define project requirements, encourage collaboration, and perform regular checks to ensure everything is baking – or, in this case, developing – smoothly. The result? A deliciously successful project!\n4. “Great Communication: The Art of Conversation”\n\n\n\n\n\nPicture project communication as a friendly chat. Be clear, concise, and approachable. Share updates regularly and encourage open dialogue. Remember, good communication is a two-way street. Listen as much as you talk, and you’ll find your project sailing smoothly.\n5. “Collaboration: Teamwork Makes the Dream Work”\n\n\n\n\n\nThink of your project team as a well-oiled machine. Encourage collaboration by creating a supportive environment. Clearly define roles, communicate openly, and celebrate achievements together. Remember, everyone plays a crucial part in the success of the project.\n\nConclusion:\nProject management doesn’t have to be a complex puzzle. Break it down into manageable steps, like assembling a Lego set. Start with the basics, manage your time wisely, focus on delivering quality, communicate openly, and foster teamwork. By simplifying project management, you’re not just ensuring success; you’re making the entire process enjoyable for everyone involved. So, grab your toolkit, put on your chef’s hat, and let’s cook up some project success together!"
  },
  {
    "objectID": "posts/Prioritize and multi task/index.html",
    "href": "posts/Prioritize and multi task/index.html",
    "title": "Mastering the Art of Task Juggling: A Simple Guide to Prioritizing and Multi-Tasking",
    "section": "",
    "text": "Mastering the Art of Task Juggling: A Simple Guide to Prioritizing and Multi-Tasking\n\nIntroduction\nHello, task-tacklers! Whether you’re a seasoned professional or just diving into the busy world of responsibilities, we’ve got your back. In this guide, we’ll unravel the secrets behind prioritizing tasks and mastering the delicate dance of multi-tasking. So, let’s make this easy to understand for everyone, because everyone deserves a stress-free to-do list!\n1. “The Magic of Prioritizing: Sorting Your To-Do List”\n\n\n\n\n\nImagine your to-do list as a garden. Some tasks are delicate flowers needing immediate attention, while others are sturdy shrubs that can wait a bit. Prioritizing is like being a gardener – identify the high-priority blooms and nurture them first. Consider deadlines, importance, and impact to decide which tasks get the sunlight of your focus.\n2. “ABCs of Prioritization: High, Medium, Low”\n\n\n\n\n\nThink of tasks in terms of urgency and importance. Label them as high, medium, or low priority. Tackle the high-priority tasks first – they’re like the VIPs in your task list, demanding immediate attention. Once the VIPs are sorted, move on to the medium and low-priority tasks. This simple ABC strategy keeps your to-do list organized and manageable.\n3. “Multi-Tasking: A Symphony of Skills”\n\n\n\n\n\nMulti-tasking is like conducting a symphony – each instrument (task) contributes to the overall harmony (completion of all tasks). Start by understanding which tasks complement each other. For example, responding to emails while waiting for a meeting to start. Keep in mind that not all tasks harmonize well together, so choose wisely.\n4. “Limit Distractions: The Peaceful Orchestra”\n\n\n\n\n\nImagine trying to conduct a symphony while fireworks go off around you. Not ideal, right? The same goes for multi-tasking. Minimize distractions to maintain focus on your tasks. Close unnecessary tabs, turn off non-urgent notifications, and create a serene environment for your task symphony to play out smoothly.\n5. “Know Your Limits: Juggling vs. Overloading”\n\n\n\n\n\nPicture multi-tasking as juggling. It’s impressive, but too many balls in the air can lead to a circus disaster. Know your limits – juggle a manageable number of tasks. It’s better to have a controlled juggling act than a chaotic mess. Quality over quantity, always.\nConclusion:\nTask management is like orchestrating a beautiful melody – it requires organization, balance, and a bit of finesse. Prioritize tasks like a skilled gardener, using the ABC strategy. When it’s time to juggle, approach multi-tasking like conducting a symphony – harmonize your tasks and limit distractions. Remember, knowing your limits is key. With these simple strategies, you’ll not only manage your tasks effectively but also enjoy the symphony of productivity. Happy task-tackling!"
  },
  {
    "objectID": "posts/Survey_guidelines/index.html",
    "href": "posts/Survey_guidelines/index.html",
    "title": "Survey tools",
    "section": "",
    "text": "Choosing the right survey tools is paramount to successful data collection. Opt for platforms like SurveyMonkey, Google Forms, or REDCap for their user-friendly interfaces and adaptability. These tools enable customization of surveys, facilitating the incorporation of context-specific questions for both household and agricultural data.\nExample:\nImagine you are collecting agricultural data on crop yields. Use SurveyMonkey to craft dynamic surveys that adjust based on respondents’ previous answers. If a farmer indicates they grow wheat, subsequent questions can automatically shift to focus on wheat-specific variables, streamlining the data collection process.\n\n\n\nEnsuring data quality in data collection tools is crucial to obtain reliable and accurate information. Here are some key practices to help ensure data quality:\n\nClear and Well-Defined Data Collection Protocols:\n\nClearly define the purpose of data collection.\nDevelop standardized data collection protocols, including detailed instructions for data collectors.\nProvide examples and guidelines for each data entry field.\n\nTraining and Capacity Building:\n\nConduct thorough training sessions for data collectors, ensuring they understand the data collection process and tools.\nInclude training on the importance of data quality, potential challenges, and how to address them.\nRegularly update the training to incorporate any changes or improvements.\n\nUse of Validated and Reliable Tools:\n\nChoose data collection tools that are validated and have a track record of reliability.\nRegularly update and patch software to address any bugs or security issues.\nConsider user-friendly interfaces to minimize errors during data entry.\n\nPre-Testing and Piloting:\n\nBefore full-scale data collection, conduct pre-testing or piloting to identify and address potential issues.\nEvaluate the effectiveness of data collection tools in a controlled environment.\n\nData Validation Checks:\n\nImplement validation checks in data collection tools to ensure the accuracy and integrity of entered data.\nUse range checks, logical checks, and consistency checks to identify and prevent errors.\n\nReal-Time Data Monitoring:\n\nMonitor incoming data in real-time to detect anomalies or inconsistencies.\nImplement automated alerts for data quality issues, enabling immediate corrective actions.\n\nStandardized Coding and Classification:\n\nUse standardized coding and classification systems to ensure consistency across data entries.\nProvide clear definitions for each code or category to minimize misinterpretation.\n\nRandom Audits and Quality Assurance Checks:\n\nConduct random audits of collected data to verify its accuracy.\nEstablish a quality assurance team to regularly review a subset of collected data for consistency and completeness.\n\nUser Feedback Mechanism:\n\nEncourage users to provide feedback on data collection tools and processes.\nEstablish a feedback mechanism to address issues reported by data collectors promptly.\n\nData Cleaning and Deduplication:\n\nRegularly perform data cleaning procedures to correct errors, inconsistencies, and missing values.\nImplement deduplication processes to identify and resolve duplicate entries.\n\nDocumentation and Metadata:\n\nDocument the data collection process, including the data dictionary, variable definitions, and any transformations applied.\nMaintain clear metadata to provide context for each dataset.\n\nRegular Review and Continuous Improvement:\n\nSchedule regular reviews of data collection processes and tools.\nContinuously seek feedback from data collectors and stakeholders to identify areas for improvement."
  },
  {
    "objectID": "posts/Monitoring_survey_data/index.html",
    "href": "posts/Monitoring_survey_data/index.html",
    "title": "Real time monitoring survey data",
    "section": "",
    "text": "Preserving the integrity of collected data demands real-time monitoring mechanisms. Employ tools like the R programming language to create scripts that automatically analyze incoming data for inconsistencies or errors.\nFirst we install the relevant packages, specifically the [testthat package](https://testthat.r-lib.org/)\n\n# Install and load necessary packages\n# install.packages(c(\"dplyr\", \"lubridate\", \"testthat\"))\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(testthat)\n\nWe then generate an example agricultural data set for practice that has “Temperature” and “Crop yield” as some of its variable. We use “Set seed” to make sure there is reproducibility of the data again.\n\n# Function to generate a sample dataset\ngenerate_sample_dataset &lt;- function() {\n  set.seed(123)\n  n_rows &lt;- 100\n  agricultural_data &lt;- data.frame(\n    temperature = rnorm(n_rows, mean = 25, sd = 5),\n    crop_yield = rpois(n_rows, lambda = 30)\n  )\n  write.csv(agricultural_data, \"agricultural_data.csv\", row.names = FALSE)\n}\n\n# Function to check for outliers and validate data\ncheck_data_quality &lt;- function() {\n  # Load the dataset\n  agricultural_data &lt;- read.csv(\"agricultural_data.csv\")\n\n  # Check for outliers in 'crop_yield' using the IQR method\n  crop_yield_outliers &lt;- boxplot.stats(agricultural_data$crop_yield)$out\n  test_that(\"No outliers in crop_yield\", expect_length(crop_yield_outliers, 0))\n\n  # Validate 'temperature' against predefined criteria\n  invalid_temperature &lt;- agricultural_data %&gt;%\n    filter(temperature &lt; -20 | temperature &gt; 40)\n  test_that(\"Valid temperature values\", expect_equal(nrow(invalid_temperature), 0))\n\n  # Add additional checks for other variables as needed\n\n  # Print timestamp for feedback\n  print(paste(\"Data quality check completed at\", Sys.time()))\n}\n\nsome text here\n\n# Generate a sample dataset\ngenerate_sample_dataset()\n\n# Schedule the script to run every day at a specific time (e.g., 2:00 AM)\nwhile (TRUE) {\n  current_time &lt;- as.numeric(format(Sys.time(), \"%H%M\"))\n\n  # Check if it's time to run the script (e.g., 2:00 AM)\n  if (current_time &gt;= 200 && current_time &lt; 201) {\n    test_file(\"data_quality_checks.R\")  # Run the tests\n    check_data_quality()\n    Sys.sleep(86400)  # Sleep for 24 hours (86400 seconds) before checking again\n  } else {\n    Sys.sleep(60)  # Sleep for 1 minute before checking again\n  }\n}\n\nIn this script, the testthat library is used to create tests within the check_data_quality function. The tests check for the absence of outliers in ‘crop_yield’ and the validity of ‘temperature’ values. If any of the tests fail, testthat will throw an error, providing instant feedback on data quality.\nThe script then schedules the data quality checks to run every day at a specific time. Adjust the time conditions as needed for your specific schedule. The Sys.sleep function is used to introduce delays between checks."
  },
  {
    "objectID": "posts/Monitoring_survey_data/index.html#monitoring-data-quality-in-real-time",
    "href": "posts/Monitoring_survey_data/index.html#monitoring-data-quality-in-real-time",
    "title": "Real time monitoring survey data",
    "section": "",
    "text": "Preserving the integrity of collected data demands real-time monitoring mechanisms. Employ tools like the R programming language to create scripts that automatically analyze incoming data for inconsistencies or errors.\nFirst we install the relevant packages, specifically the [testthat package](https://testthat.r-lib.org/)\n\n# Install and load necessary packages\n# install.packages(c(\"dplyr\", \"lubridate\", \"testthat\"))\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(testthat)\n\nWe then generate an example agricultural data set for practice that has “Temperature” and “Crop yield” as some of its variable. We use “Set seed” to make sure there is reproducibility of the data again.\n\n# Function to generate a sample dataset\ngenerate_sample_dataset &lt;- function() {\n  set.seed(123)\n  n_rows &lt;- 100\n  agricultural_data &lt;- data.frame(\n    temperature = rnorm(n_rows, mean = 25, sd = 5),\n    crop_yield = rpois(n_rows, lambda = 30)\n  )\n  write.csv(agricultural_data, \"agricultural_data.csv\", row.names = FALSE)\n}\n\n# Function to check for outliers and validate data\ncheck_data_quality &lt;- function() {\n  # Load the dataset\n  agricultural_data &lt;- read.csv(\"agricultural_data.csv\")\n\n  # Check for outliers in 'crop_yield' using the IQR method\n  crop_yield_outliers &lt;- boxplot.stats(agricultural_data$crop_yield)$out\n  test_that(\"No outliers in crop_yield\", expect_length(crop_yield_outliers, 0))\n\n  # Validate 'temperature' against predefined criteria\n  invalid_temperature &lt;- agricultural_data %&gt;%\n    filter(temperature &lt; -20 | temperature &gt; 40)\n  test_that(\"Valid temperature values\", expect_equal(nrow(invalid_temperature), 0))\n\n  # Add additional checks for other variables as needed\n\n  # Print timestamp for feedback\n  print(paste(\"Data quality check completed at\", Sys.time()))\n}\n\nsome text here\n\n# Generate a sample dataset\ngenerate_sample_dataset()\n\n# Schedule the script to run every day at a specific time (e.g., 2:00 AM)\nwhile (TRUE) {\n  current_time &lt;- as.numeric(format(Sys.time(), \"%H%M\"))\n\n  # Check if it's time to run the script (e.g., 2:00 AM)\n  if (current_time &gt;= 200 && current_time &lt; 201) {\n    test_file(\"data_quality_checks.R\")  # Run the tests\n    check_data_quality()\n    Sys.sleep(86400)  # Sleep for 24 hours (86400 seconds) before checking again\n  } else {\n    Sys.sleep(60)  # Sleep for 1 minute before checking again\n  }\n}\n\nIn this script, the testthat library is used to create tests within the check_data_quality function. The tests check for the absence of outliers in ‘crop_yield’ and the validity of ‘temperature’ values. If any of the tests fail, testthat will throw an error, providing instant feedback on data quality.\nThe script then schedules the data quality checks to run every day at a specific time. Adjust the time conditions as needed for your specific schedule. The Sys.sleep function is used to introduce delays between checks."
  },
  {
    "objectID": "posts/Monitoring_survey_data/index.html#summary",
    "href": "posts/Monitoring_survey_data/index.html#summary",
    "title": "Real time monitoring survey data Using R",
    "section": "Summary",
    "text": "Summary\nIn this script, the testthat library is used to create tests within the check_data_quality function. The tests check for the absence of outliers in ‘crop_yield’ and the validity of ‘temperature’ values. If any of the tests fail, testthat will throw an error, providing instant feedback on data quality.\nThe script then schedules the data quality checks to run every day at a specific time. Adjust the time conditions as needed for your specific schedule. The Sys.sleep function is used to introduce delays between checks."
  },
  {
    "objectID": "posts/Training_data_collection/index.html",
    "href": "posts/Training_data_collection/index.html",
    "title": "Training staff",
    "section": "",
    "text": "The competence of your field staff is pivotal to the success of your data collection program. When recruiting, prioritize individuals with a blend of domain expertise and technological proficiency. Ensure comprehensive training sessions that cover both survey tools and the intricacies of the data being collected.\n\n\nObjective: To simulate the process of using survey tools for data collection, specifically Google Forms, and to familiarize staff with the survey creation, distribution, and data retrieval processes.\nScenario: Imagine you are part of an agricultural organization tasked with collecting data on fruit preferences from participants attending a farmers’ conference. The goal is to understand the most popular fruits among farmers for potential agricultural planning.\nSteps:\n\nCreate a Mock Survey:\n\nCreate a Google Form with questions related to fruit preferences. Include multiple-choice questions, checkboxes, and a text entry for comments.\nExample Questions:\n\nWhich fruit do you prefer the most? (Multiple-choice: Apple, Orange, Banana, Other)\nDo you grow any fruits on your farm? (Checkbox: Apple, Orange, Banana, Mango, Other)\nAny additional comments or suggestions?\n\n\nShare the Mock Survey:\n\nShare the Google Form with the training participants using a unique link.\nIn a real-world scenario, this link could be distributed via email, QR codes, or any other method.\n\nSimulate Data Collection:\n\nIn the training session, ask participants to fill out the mock survey using the provided link.\nEncourage them to use different devices such as smartphones, tablets, or laptops to simulate real-world conditions.\n\nReview Survey Responses:\n\nDemonstrate how to access and review survey responses in real-time.\nDiscuss how to analyze the collected data within the Google Forms interface.\n\nTroubleshooting Exercise:\n\nIntroduce common issues that may arise during data collection, such as incomplete submissions, errors in responses, or technical difficulties.\nEncourage participants to troubleshoot and find solutions collaboratively.\n\nFeedback and Discussion:\n\nFacilitate a discussion on the challenges faced during the mock data collection exercise.\nEncourage participants to share insights and lessons learned.\nProvide tips on improving the data collection process.\n\n\nBenefits:\n\nFamiliarity: Participants gain hands-on experience using the survey tool, making them more comfortable with its features.\nTroubleshooting Skills: Staff learn to identify and address issues that may arise during real data collection.\nTeam Collaboration: The exercise fosters teamwork as participants work together to solve challenges.\nFeedback Loop: Insights from the exercise can be used to enhance training materials and improve the actual data collection process.\n\nBy incorporating a mock data collection exercise into training, staff members can build practical skills and confidence, ensuring a smoother transition to actual fieldwork."
  },
  {
    "objectID": "posts/Training_data_collection/index.html#recruitment-and-training-of-data-collection-staff",
    "href": "posts/Training_data_collection/index.html#recruitment-and-training-of-data-collection-staff",
    "title": "Training staff",
    "section": "",
    "text": "The competence of your field staff is pivotal to the success of your data collection program. When recruiting, prioritize individuals with a blend of domain expertise and technological proficiency. Ensure comprehensive training sessions that cover both survey tools and the intricacies of the data being collected.\n\n\nObjective: To simulate the process of using survey tools for data collection, specifically Google Forms, and to familiarize staff with the survey creation, distribution, and data retrieval processes.\nScenario: Imagine you are part of an agricultural organization tasked with collecting data on fruit preferences from participants attending a farmers’ conference. The goal is to understand the most popular fruits among farmers for potential agricultural planning.\nSteps:\n\nCreate a Mock Survey:\n\nCreate a Google Form with questions related to fruit preferences. Include multiple-choice questions, checkboxes, and a text entry for comments.\nExample Questions:\n\nWhich fruit do you prefer the most? (Multiple-choice: Apple, Orange, Banana, Other)\nDo you grow any fruits on your farm? (Checkbox: Apple, Orange, Banana, Mango, Other)\nAny additional comments or suggestions?\n\n\nShare the Mock Survey:\n\nShare the Google Form with the training participants using a unique link.\nIn a real-world scenario, this link could be distributed via email, QR codes, or any other method.\n\nSimulate Data Collection:\n\nIn the training session, ask participants to fill out the mock survey using the provided link.\nEncourage them to use different devices such as smartphones, tablets, or laptops to simulate real-world conditions.\n\nReview Survey Responses:\n\nDemonstrate how to access and review survey responses in real-time.\nDiscuss how to analyze the collected data within the Google Forms interface.\n\nTroubleshooting Exercise:\n\nIntroduce common issues that may arise during data collection, such as incomplete submissions, errors in responses, or technical difficulties.\nEncourage participants to troubleshoot and find solutions collaboratively.\n\nFeedback and Discussion:\n\nFacilitate a discussion on the challenges faced during the mock data collection exercise.\nEncourage participants to share insights and lessons learned.\nProvide tips on improving the data collection process.\n\n\nBenefits:\n\nFamiliarity: Participants gain hands-on experience using the survey tool, making them more comfortable with its features.\nTroubleshooting Skills: Staff learn to identify and address issues that may arise during real data collection.\nTeam Collaboration: The exercise fosters teamwork as participants work together to solve challenges.\nFeedback Loop: Insights from the exercise can be used to enhance training materials and improve the actual data collection process.\n\nBy incorporating a mock data collection exercise into training, staff members can build practical skills and confidence, ensuring a smoother transition to actual fieldwork."
  },
  {
    "objectID": "posts/Survey_guidelines/index.html#selecting-program-survey-tools",
    "href": "posts/Survey_guidelines/index.html#selecting-program-survey-tools",
    "title": "Survey tools",
    "section": "",
    "text": "Choosing the right survey tools is paramount to successful data collection. Opt for platforms like SurveyMonkey, Google Forms, or REDCap for their user-friendly interfaces and adaptability. These tools enable customization of surveys, facilitating the incorporation of context-specific questions for both household and agricultural data.\nExample:\nImagine you are collecting agricultural data on crop yields. Use SurveyMonkey to craft dynamic surveys that adjust based on respondents’ previous answers. If a farmer indicates they grow wheat, subsequent questions can automatically shift to focus on wheat-specific variables, streamlining the data collection process.\n\n\n\nEnsuring data quality in data collection tools is crucial to obtain reliable and accurate information. Here are some key practices to help ensure data quality:\n\nClear and Well-Defined Data Collection Protocols:\n\nClearly define the purpose of data collection.\nDevelop standardized data collection protocols, including detailed instructions for data collectors.\nProvide examples and guidelines for each data entry field.\n\nTraining and Capacity Building:\n\nConduct thorough training sessions for data collectors, ensuring they understand the data collection process and tools.\nInclude training on the importance of data quality, potential challenges, and how to address them.\nRegularly update the training to incorporate any changes or improvements.\n\nUse of Validated and Reliable Tools:\n\nChoose data collection tools that are validated and have a track record of reliability.\nRegularly update and patch software to address any bugs or security issues.\nConsider user-friendly interfaces to minimize errors during data entry.\n\nPre-Testing and Piloting:\n\nBefore full-scale data collection, conduct pre-testing or piloting to identify and address potential issues.\nEvaluate the effectiveness of data collection tools in a controlled environment.\n\nData Validation Checks:\n\nImplement validation checks in data collection tools to ensure the accuracy and integrity of entered data.\nUse range checks, logical checks, and consistency checks to identify and prevent errors.\n\nReal-Time Data Monitoring:\n\nMonitor incoming data in real-time to detect anomalies or inconsistencies.\nImplement automated alerts for data quality issues, enabling immediate corrective actions.\n\nStandardized Coding and Classification:\n\nUse standardized coding and classification systems to ensure consistency across data entries.\nProvide clear definitions for each code or category to minimize misinterpretation.\n\nRandom Audits and Quality Assurance Checks:\n\nConduct random audits of collected data to verify its accuracy.\nEstablish a quality assurance team to regularly review a subset of collected data for consistency and completeness.\n\nUser Feedback Mechanism:\n\nEncourage users to provide feedback on data collection tools and processes.\nEstablish a feedback mechanism to address issues reported by data collectors promptly.\n\nData Cleaning and Deduplication:\n\nRegularly perform data cleaning procedures to correct errors, inconsistencies, and missing values.\nImplement deduplication processes to identify and resolve duplicate entries.\n\nDocumentation and Metadata:\n\nDocument the data collection process, including the data dictionary, variable definitions, and any transformations applied.\nMaintain clear metadata to provide context for each dataset.\n\nRegular Review and Continuous Improvement:\n\nSchedule regular reviews of data collection processes and tools.\nContinuously seek feedback from data collectors and stakeholders to identify areas for improvement."
  },
  {
    "objectID": "posts/organizing-survey-logistics/index.html",
    "href": "posts/organizing-survey-logistics/index.html",
    "title": "Organizing Survey logistics",
    "section": "",
    "text": "Efficient logistics are a cornerstone for the seamless execution of data collection initiatives. Plan and schedule surveys strategically to optimize resources and minimize disruptions. Leverage project management tools such as Trello or Asana to coordinate field staff and track progress.\n\n\nBoard Title: Data Collection Survey Schedule\nLists:\n\nTo-Do:\n\nCard 1: Prepare Survey Questions\nCard 2: Create Google Form\nCard 3: Set Up Trello Board\nCard 4: Define Data Collection Period\n\nIn Progress:\n\nCard 5: Share Google Form Link\nCard 6: Conduct Mock Data Collection Exercise\n\nData Collection - Phase 1:\n\nCard 7: Distribute Survey to Farmers in Region A\nCard 8: Monitor Responses in Real-Time\nCard 9: Address Any Technical Issues\n\nData Collection - Phase 2:\n\nCard 10: Distribute Survey to Farmers in Region B\nCard 11: Monitor Responses in Real-Time\nCard 12: Address Any Technical Issues\n\nData Collection - Phase 3:\n\nCard 13: Distribute Survey to Farmers in Region C\nCard 14: Monitor Responses in Real-Time\nCard 15: Address Any Technical Issues\n\nData Analysis:\n\nCard 16: Analyze Survey Results\nCard 17: Generate Summary Report\n\n\nTeam Member Assignments:\n\nJohn (Project Manager):\n\nCards 1-4\nOversee the entire project and ensure all tasks are on schedule.\n\nAlice (Survey Coordinator):\n\nCards 5-6\nCoordinate the mock data collection exercise and share the survey link.\n\nBob (Field Team - Region A):\n\nCards 7-9\nDistribute surveys and monitor responses in Region A.\n\nCharlie (Field Team - Region B):\n\nCards 10-12\nDistribute surveys and monitor responses in Region B.\n\nDiana (Field Team - Region C):\n\nCards 13-15\nDistribute surveys and monitor responses in Region C.\n\nEva (Data Analyst):\n\nCards 16-17\nAnalyze survey results and generate a summary report.\n\n\nBoard Overview:\n\nTeam members can move their assigned cards across lists as they progress through different stages.\nThe board provides a clear visual representation of the survey schedule and each team member’s responsibilities.\nReal-time updates on Trello enable everyone to track progress and identify any bottlenecks.\n\nBenefits:\n\nVisual Representation: Team members can quickly grasp the project’s status and upcoming tasks.\nReal-Time Monitoring: Progress is updated in real-time, enhancing transparency and accountability.\nEfficient Collaboration: Team members can leave comments on cards for discussions and updates.\nIdentifying Bottlenecks: Delays or issues can be easily identified by tracking the movement of cards.\n\nThis Trello board example facilitates effective team coordination, ensuring that each stage of the survey schedule is managed efficiently and promoting accountability within the team. Adjust the structure based on the specific needs and complexity of your data collection project."
  },
  {
    "objectID": "posts/organizing-survey-logistics/index.html#organizing-survey-logistics",
    "href": "posts/organizing-survey-logistics/index.html#organizing-survey-logistics",
    "title": "Organizing Survey logistics",
    "section": "",
    "text": "Efficient logistics are a cornerstone for the seamless execution of data collection initiatives. Plan and schedule surveys strategically to optimize resources and minimize disruptions. Leverage project management tools such as Trello or Asana to coordinate field staff and track progress.\n\n\nBoard Title: Data Collection Survey Schedule\nLists:\n\nTo-Do:\n\nCard 1: Prepare Survey Questions\nCard 2: Create Google Form\nCard 3: Set Up Trello Board\nCard 4: Define Data Collection Period\n\nIn Progress:\n\nCard 5: Share Google Form Link\nCard 6: Conduct Mock Data Collection Exercise\n\nData Collection - Phase 1:\n\nCard 7: Distribute Survey to Farmers in Region A\nCard 8: Monitor Responses in Real-Time\nCard 9: Address Any Technical Issues\n\nData Collection - Phase 2:\n\nCard 10: Distribute Survey to Farmers in Region B\nCard 11: Monitor Responses in Real-Time\nCard 12: Address Any Technical Issues\n\nData Collection - Phase 3:\n\nCard 13: Distribute Survey to Farmers in Region C\nCard 14: Monitor Responses in Real-Time\nCard 15: Address Any Technical Issues\n\nData Analysis:\n\nCard 16: Analyze Survey Results\nCard 17: Generate Summary Report\n\n\nTeam Member Assignments:\n\nJohn (Project Manager):\n\nCards 1-4\nOversee the entire project and ensure all tasks are on schedule.\n\nAlice (Survey Coordinator):\n\nCards 5-6\nCoordinate the mock data collection exercise and share the survey link.\n\nBob (Field Team - Region A):\n\nCards 7-9\nDistribute surveys and monitor responses in Region A.\n\nCharlie (Field Team - Region B):\n\nCards 10-12\nDistribute surveys and monitor responses in Region B.\n\nDiana (Field Team - Region C):\n\nCards 13-15\nDistribute surveys and monitor responses in Region C.\n\nEva (Data Analyst):\n\nCards 16-17\nAnalyze survey results and generate a summary report.\n\n\nBoard Overview:\n\nTeam members can move their assigned cards across lists as they progress through different stages.\nThe board provides a clear visual representation of the survey schedule and each team member’s responsibilities.\nReal-time updates on Trello enable everyone to track progress and identify any bottlenecks.\n\nBenefits:\n\nVisual Representation: Team members can quickly grasp the project’s status and upcoming tasks.\nReal-Time Monitoring: Progress is updated in real-time, enhancing transparency and accountability.\nEfficient Collaboration: Team members can leave comments on cards for discussions and updates.\nIdentifying Bottlenecks: Delays or issues can be easily identified by tracking the movement of cards.\n\nThis Trello board example facilitates effective team coordination, ensuring that each stage of the survey schedule is managed efficiently and promoting accountability within the team. Adjust the structure based on the specific needs and complexity of your data collection project."
  },
  {
    "objectID": "posts/01_practical_analysis/index.html",
    "href": "posts/01_practical_analysis/index.html",
    "title": "Potential causes of repetition and dropout in Primary Education covering Primary 1 (P1) to Primary 6 case study: Rural and Urban areas(P6)",
    "section": "",
    "text": "This document is presents the report from analysis conducted on Primary Education for the Ministry of Education (MINEDUC) in Rwanda. This report mainly focuses on rural and urban areas in\nsource of the data: here\nThe code below illustrates on how to load packages needed for the analysis\n#load package\nlibrary(tidyverse)\nlibrary(tidyr)\nlibrary(broom)\nlibrary(purrr)\nThen we read in the data using the link to avoid wasting space and increase spead using tidyverse read_csv\ndata &lt;- read_csv(\"https://raw.githubusercontent.com/vmandela99/laterite-interview/master/laterite_education_data.csv\" )"
  },
  {
    "objectID": "posts/01_practical_analysis/index.html#the-task",
    "href": "posts/01_practical_analysis/index.html#the-task",
    "title": "Potential causes of repetition and dropout in Primary Education covering Primary 1 (P1) to Primary 6 case study: Rural and Urban areas(P6)",
    "section": "",
    "text": "This document is presents the report from analysis conducted on Primary Education for the Ministry of Education (MINEDUC) in Rwanda. This report mainly focuses on rural and urban areas in\nsource of the data: here\nThe code below illustrates on how to load packages needed for the analysis\n#load package\nlibrary(tidyverse)\nlibrary(tidyr)\nlibrary(broom)\nlibrary(purrr)\nThen we read in the data using the link to avoid wasting space and increase spead using tidyverse read_csv\ndata &lt;- read_csv(\"https://raw.githubusercontent.com/vmandela99/laterite-interview/master/laterite_education_data.csv\" )"
  },
  {
    "objectID": "posts/01_practical_analysis/index.html#introduction",
    "href": "posts/01_practical_analysis/index.html#introduction",
    "title": "Potential causes of repetition and dropout in Primary Education covering Primary 1 (P1) to Primary 6 case study: Rural and Urban areas(P6)",
    "section": "Introduction",
    "text": "Introduction\nThe first task would be to clean the names, check for missing values, undestand the column names, check the data types and, also make sure that the data is in tidy format.\nThe following code is for renaming the columns for the to have meaningful names\n## rename the variables\nnames(data)\nr_data &lt;- data %&gt;% rename(Sex = s1q1, \n                          Age=s1q3y, \n                          region_class = ur2012,\n                          Father_alive=s1q13,\n                          Mother_alive = s1q14, \n                          health_prob=s3q4,\n                          Grade_2012=s4aq6a ,\n                          Grade_2013=s4aq6b , \n                          sch_attended_prev_yr=s4aq8, \n                          prob_in_sch=s4aq9,\n                          edu_expenses=s4aq11h,                         paid_edu_expenses_year_end=s4aq12 , \n                          sch_days_missed=s4aq14, \n                          why_not_attending_sch=s4aq15,\n                          why_leave_sch=s4aq17,\n                          can_read=s4bq3,\n                          can_write=s4bq4,\n                          can_calculate=s4bq5,\n                          farm_work=s6aq2)\n\nThen we define the missing values as NAs then remove them from the two variables. But this is always advisable after you have inquired from other departments why the data is missing in the first place. Beware that some missing data can not just be deleted, instead there are a couple of imputing techniques that we discuss the upcoming blogs. For now we illustrate how to delete them.\ndata &lt;- data %&gt;% \n  na_if(\"\") %&gt;% \n  filter(!is.na(Grade_2012),!is.na(Grade_2013))\nThis report investigates the causes of repetiton in primary education. Rural area here mean that the setting of the school location has low standards of living status and low population to infrastructure ratio while urban is the opposite."
  },
  {
    "objectID": "posts/01_practical_analysis/index.html#descriptive-analysis",
    "href": "posts/01_practical_analysis/index.html#descriptive-analysis",
    "title": "Potential causes of repetition and dropout in Primary Education covering Primary 1 (P1) to Primary 6 case study: Rural and Urban areas(P6)",
    "section": "Descriptive analysis",
    "text": "Descriptive analysis\n\nProvinces\nThe report was mainly done in Rwanda where 5 provinces were considered. The provinces were Kigali city, Southern province, Western province, Northern Province and Eastern province. The table below summarises the percentage distribution of students from each province. Kigali city had 22.15 percent which was the highest numbers from a province in this study. However, the rest of the provinces had a nearly similar number with Southern province having the lowest number of students at 17.1 percent.\nThis is R code used to produce the table\ntable(r_data$province)-&gt;tabb\nprop.table(tabb)*100-&gt;tabb1\ntabb1%&gt;% knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVar1\n\nFreq\n\n\n\n\n\n\n\n\n\n\n\nKigali City\n\n22.15247\n\n\n\n\n\n\n\n\n\n\n\nSouthern Province\n\n17.10015\n\n\n\n\n\n\n\n\n\n\n\nWestern Province\n\n21.13602\n\n\n\n\n\n\n\n\n\n\n\nNorthern Province\n\n18.83408\n\n\n\n\n\n\n\n\n\n\n\nEastern Province\n\n20.77728\n\n\n\n\n\n\n\n\n\n\n\n\n\nDistricts\nThe report also looked at the following districts in Rwanda which are; Nyarugenge,Gasabo,Kicukiro,Nyanza,Gisagara,Nyaruguru,HuyeNyamagabe,Ruhango,Muhanga,Kamonyi,Karongi,RutsiroRubavu,Nyabihu,Ngororero,Rusizi,Nyamasheke,Rulindo,Gakenke,Musanze,Burera,Gicumbi,Rwamagana,Nyagatare,Gatsibo,Kayonza,Kirehe,Ngoma,Bugesera. The table below summarises the percentage distribution of students from each district. For this study, Gesabu had the highest number of student, 357 and Huye had the lowest number, which is 31 students.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVar1\n\nFreq\n\n\n\n\n\n\n\n\n\n\n\nNyarugenge\n\n133\n\n\n\n\n\n\n\n\n\n\n\nGasabo\n\n357\n\n\n\n\n\n\n\n\n\n\n\nKicukiro\n\n251\n\n\n\n\n\n\n\n\n\n\n\nNyanza\n\n43\n\n\n\n\n\n\n\n\n\n\n\nGisagara\n\n122\n\n\n\n\n\n\n\n\n\n\n\nNyaruguru\n\n125\n\n\n\n\n\n\n\n\n\n\n\nHuye\n\n31\n\n\n\n\n\n\n\n\n\n\n\nNyamagabe\n\n66\n\n\n\n\n\n\n\n\n\n\n\nRuhango\n\n82\n\n\n\n\n\n\n\n\n\n\n\nMuhanga\n\n61\n\n\n\n\n\n\n\n\n\n\n\nKamonyi\n\n42\n\n\n\n\n\n\n\n\n\n\n\nKarongi\n\n86\n\n\n\n\n\n\n\n\n\n\n\nRutsiro\n\n62\n\n\n\n\n\n\n\n\n\n\n\nRubavu\n\n72\n\n\n\n\n\n\n\n\n\n\n\nNyabihu\n\n110\n\n\n\n\n\n\n\n\n\n\n\nNgororero\n\n109\n\n\n\n\n\n\n\n\n\n\n\nRusizi\n\n92\n\n\n\n\n\n\n\n\n\n\n\nNyamasheke\n\n176\n\n\n\n\n\n\n\n\n\n\n\nRulindo\n\n75\n\n\n\n\n\n\n\n\n\n\n\nGakenke\n\n162\n\n\n\n\n\n\n\n\n\n\n\nMusanze\n\n104\n\n\n\n\n\n\n\n\n\n\n\nBurera\n\n179\n\n\n\n\n\n\n\n\n\n\n\nGicumbi\n\n110\n\n\n\n\n\n\n\n\n\n\n\nRwamagana\n\n58\n\n\n\n\n\n\n\n\n\n\n\nNyagatare\n\n127\n\n\n\n\n\n\n\n\n\n\n\nGatsibo\n\n88\n\n\n\n\n\n\n\n\n\n\n\nKayonza\n\n114\n\n\n\n\n\n\n\n\n\n\n\nKirehe\n\n106\n\n\n\n\n\n\n\n\n\n\n\nNgoma\n\n145\n\n\n\n\n\n\n\n\n\n\n\nBugesera\n\n57\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpread by region\nThe study divided region into four regions depending on the economic and development status. The regions considered in this study include;- Urban, rural, semi-urban and peri-urban regions. The table below show that the highest number was from the rural region having 75,5 percent of the number of students in this study. This shows that the researcher chose higher samples from the population from the rural set-up which might be suspected to have high turn over of repeating in grades.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVar1\n\nFreq\n\n\n\n\n\n\n\n\n\n\n\nPeri urban\n\n15.6950673\n\n\n\n\n\n\n\n\n\n\n\nUrban\n\n8.0119581\n\n\n\n\n\n\n\n\n\n\n\nRural\n\n75.4559043\n\n\n\n\n\n\n\n\n\n\n\nSemi urban\n\n0.8370703\n\n\n\n\n\n\n\n\n\n\n\n\n\nGender Distribution\nThe study tried to sample an equal number of students in respect to gender. This is shown by the table below where the ratio of famales to men was almost one to one.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVar1\n\nFreq\n\n\n\n\n\n\n\n\n\n\n\nFemale\n\n50.9417\n\n\n\n\n\n\n\n\n\n\n\nMale\n\n49.0583"
  },
  {
    "objectID": "posts/01_practical_analysis/index.html#analysis",
    "href": "posts/01_practical_analysis/index.html#analysis",
    "title": "Potential causes of repetition and dropout in Primary Education covering Primary 1 (P1) to Primary 6 case study: Rural and Urban areas(P6)",
    "section": "Analysis",
    "text": "Analysis\n\nRepetition within grades in Primary Education\nThis analysis shows the findings of how repetition in grades in primary school varies across grades in school. It can be seen that at the time of study, apart from primary 1 having the highest number of students, 39.5 percent of the 686 pupils in that class had actually repeated the same grade from 2012. The other classes with the highest repetition rate are primary 2 (23.6 percent of 470), primary 5 (22.3 percent of 260), primary 3 (16.8 percent of 392) and primary 4 (16.7 percent of 305). It is also worthy noting that from post primary 1 to post primary 5 there was no cases of repetiton from 2012.\nThe R code of producing this is\n## how grade repetition varies by grade in Primary Education \ncomparis &lt;- data %&gt;% filter(!(Grade_2012%in%c(\"Not in class\")))\ntable(comparis$Grade_2012,comparis$repeated)-&gt;comparison_repetition_in_classes_2012\nprop.table(comparison_repetition_in_classes_2012,1)*100-&gt;tabwew\ntabwew %&gt;% knitr::kable()\n\nggplot(comparis, aes(x=repeated))+ geom_bar(position = \"dodge\")+facet_wrap(~Grade_2012)\nggplot(comparis, aes(x=Grade_2012,fill =repeated))+ geom_bar(position = \"stack\")+coord_flip()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFALSE\n\nTRUE\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost primary 1\n\n100.00000\n\n0.000000\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost primary 3\n\n100.00000\n\n0.000000\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost primary 4\n\n100.00000\n\n0.000000\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost primary 5\n\n100.00000\n\n0.000000\n\n\n\n\n\n\n\n\n\n\n\n\n\nPre-primary\n\n95.88235\n\n4.117647\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrimary 1\n\n60.49563\n\n39.504373\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrimary 2\n\n76.38298\n\n23.617021\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrimary 3\n\n83.16327\n\n16.836735\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrimary 4\n\n83.27869\n\n16.721311\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrimary 5\n\n77.69231\n\n22.307692\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrimary 6,7,8\n\n93.19728\n\n6.802721\n\n\n\n\n\n\n\n\n\n\n\n\n\nSecondary 1\n\n97.67442\n\n2.325581\n\n\n\n\n\n\n\n\n\n\n\n\n\nSecondary 2\n\n90.56604\n\n9.433962\n\n\n\n\n\n\n\n\n\n\n\n\n\nSecondary 3\n\n94.11765\n\n5.882353\n\n\n\n\n\n\n\n\n\n\n\n\n\nSecondary 4\n\n100.00000\n\n0.000000\n\n\n\n\n\n\n\n\n\n\n\n\n\nSecondary 5\n\n100.00000\n\n0.000000\n\n\n\n\n\n\n\n\n\n\n\n\n\nSecondary 6\n\n100.00000\n\n0.000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMales equally likely to drop out as females.\nThe research also wanted to check which gender had a higher drop out rate. The results showed a comparisons which was not significate between the two genders (since t.test for which variance is same showes a p-value of 0.9765 using Welch two sample test, which is &gt; 0.05). This shows that the two means of the genders were almost equal and therefore the conclusion would be that both male and female pupils had equal chances of dropping out from school.\n\n\nRegression analysis\nIn with the aim of investigating the determinants contributing to increase in rate of repetition, the researcher opted to consider the following predictor variables;- the weight, age, whether the father or mother was alive or not, the health problems suffered in the last 4 weeks, grade attended in during 2012 and 2013, who paid for the student expenses for the last 12 months and the reason why they(pupils who missed) didnt attend school. The response variable would be repeating a grade in school which would be binary,where 1 would mean repeated is true and 0 if otherwise. A binary logistic regression model was used. The predictor with p-value that were less than 0.05 were reported as significant.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregion_class\n\n\n\n\n\n\n\n\n\n\n\n\nterm\n\n\n\n\n\n\n\n\n\n\n\n\nestimate\n\n\n\n\n\n\n\n\n\n\n\n\nstd.error\n\n\n\n\n\n\n\n\n\n\n\n\np.value\n\n\n\n\n\n\n\n\n\n\n\n\np.adjusted\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRural\n\n\n\n\n\n\n\n\n\n\n\n\nGrade_2012Primary 5\n\n\n\n\n\n\n\n\n\n\n\n\n2.6246517\n\n\n\n\n\n\n\n\n\n\n\n\n0.1333536\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000000\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRural\n\n\n\n\n\n\n\n\n\n\n\n\nGrade_2012Primary 6,7,8\n\n\n\n\n\n\n\n\n\n\n\n\n2.3926222\n\n\n\n\n\n\n\n\n\n\n\n\n0.1510041\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000000\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRural\n\n\n\n\n\n\n\n\n\n\n\n\nGrade_2012Primary 4\n\n\n\n\n\n\n\n\n\n\n\n\n2.2149811\n\n\n\n\n\n\n\n\n\n\n\n\n0.1142086\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000000\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRural\n\n\n\n\n\n\n\n\n\n\n\n\nGrade_2012Primary 3\n\n\n\n\n\n\n\n\n\n\n\n\n1.8104449\n\n\n\n\n\n\n\n\n\n\n\n\n0.0960966\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000000\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRural\n\n\n\n\n\n\n\n\n\n\n\n\nGrade_2012Primary 2\n\n\n\n\n\n\n\n\n\n\n\n\n1.3240467\n\n\n\n\n\n\n\n\n\n\n\n\n0.0711579\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000000\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRural\n\n\n\n\n\n\n\n\n\n\n\n\nGrade_2012Primary 1\n\n\n\n\n\n\n\n\n\n\n\n\n0.7385963\n\n\n\n\n\n\n\n\n\n\n\n\n0.0481840\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000000\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRural\n\n\n\n\n\n\n\n\n\n\n\n\nprob_in_schMediocre teaching\n\n\n\n\n\n\n\n\n\n\n\n\n-0.3153225\n\n\n\n\n\n\n\n\n\n\n\n\n0.0858360\n\n\n\n\n\n\n\n\n\n\n\n\n0.0002598\n\n\n\n\n\n\n\n\n\n\n\n\n0.0220845\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRural\n\n\n\n\n\n\n\n\n\n\n\n\nGrade_2013Pre-primary\n\n\n\n\n\n\n\n\n\n\n\n\n-0.5500117\n\n\n\n\n\n\n\n\n\n\n\n\n0.1059252\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000003\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000259\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRural\n\n\n\n\n\n\n\n\n\n\n\n\nGrade_2013Primary 1\n\n\n\n\n\n\n\n\n\n\n\n\n-0.7292078\n\n\n\n\n\n\n\n\n\n\n\n\n0.0599060\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000000\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRural\n\n\n\n\n\n\n\n\n\n\n\n\nGrade_2013Secondary 4\n\n\n\n\n\n\n\n\n\n\n\n\n-0.8776014\n\n\n\n\n\n\n\n\n\n\n\n\n0.1657405\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000002\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000154\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRural\n\n\n\n\n\n\n\n\n\n\n\n\nGrade_2013Primary 2\n\n\n\n\n\n\n\n\n\n\n\n\n-1.4516728\n\n\n\n\n\n\n\n\n\n\n\n\n0.0739026\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000000\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRural\n\n\n\n\n\n\n\n\n\n\n\n\nGrade_2012Secondary 1\n\n\n\n\n\n\n\n\n\n\n\n\n-1.9269528\n\n\n\n\n\n\n\n\n\n\n\n\n0.4238364\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000066\n\n\n\n\n\n\n\n\n\n\n\n\n0.0005706\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRural\n\n\n\n\n\n\n\n\n\n\n\n\nGrade_2013Primary 3\n\n\n\n\n\n\n\n\n\n\n\n\n-2.0677477\n\n\n\n\n\n\n\n\n\n\n\n\n0.0893855\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000000\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRural\n\n\n\n\n\n\n\n\n\n\n\n\nGrade_2013Primary 4\n\n\n\n\n\n\n\n\n\n\n\n\n-2.5562536\n\n\n\n\n\n\n\n\n\n\n\n\n0.1099310\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000000\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRural\n\n\n\n\n\n\n\n\n\n\n\n\nGrade_2013Primary 5\n\n\n\n\n\n\n\n\n\n\n\n\n-2.9426288\n\n\n\n\n\n\n\n\n\n\n\n\n0.1245526\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000000\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRural\n\n\n\n\n\n\n\n\n\n\n\n\nGrade_2013Secondary 1\n\n\n\n\n\n\n\n\n\n\n\n\n-3.1527569\n\n\n\n\n\n\n\n\n\n\n\n\n0.1589982\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000000\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRural\n\n\n\n\n\n\n\n\n\n\n\n\nGrade_2013Primary 6,7,8\n\n\n\n\n\n\n\n\n\n\n\n\n-3.4075162\n\n\n\n\n\n\n\n\n\n\n\n\n0.1413879\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000000\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe table above show the findings of analysis from the rural area in rwanda where only problems experienced in school, grade in 2012 and 2013 were significant, we took . It shows that in 2012, pupils in primary 5,(6 to 8),4,3,2 and 1 were 13.73, 10.91, 9.12, 6.11, 3.74, 2.09 times more likely to repeat the same grade, in that order, than the students in post primary 1. Other hand, in 2013, pupils in pre-primary were 27 percent less likely to repeat the same grade as compared to those in post primary 1 in 2013. Further, those in primary 6, 7, 8 in 2013, were the least likely to repeat, i.e, 96.7 percent less likely to repeat as compared to post primary 1 in 2013.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregion_class\n\n\n\n\n\n\n\n\n\n\n\n\nterm\n\n\n\n\n\n\n\n\n\n\n\n\nestimate\n\n\n\n\n\n\n\n\n\n\n\n\nstd.error\n\n\n\n\n\n\n\n\n\n\n\n\np.value\n\n\n\n\n\n\n\n\n\n\n\n\np.adjusted\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUrban\n\n\n\n\n\n\n\n\n\n\n\n\nGrade_2012Primary 5\n\n\n\n\n\n\n\n\n\n\n\n\n2.531815\n\n\n\n\n\n\n\n\n\n\n\n\n0.3913698\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000000\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUrban\n\n\n\n\n\n\n\n\n\n\n\n\nGrade_2012Primary 4\n\n\n\n\n\n\n\n\n\n\n\n\n1.737260\n\n\n\n\n\n\n\n\n\n\n\n\n0.3011179\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000003\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000270\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUrban\n\n\n\n\n\n\n\n\n\n\n\n\nGrade_2012Primary 3\n\n\n\n\n\n\n\n\n\n\n\n\n1.507446\n\n\n\n\n\n\n\n\n\n\n\n\n0.2891581\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000024\n\n\n\n\n\n\n\n\n\n\n\n\n0.0002122\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUrban\n\n\n\n\n\n\n\n\n\n\n\n\nGrade_2012Primary 2\n\n\n\n\n\n\n\n\n\n\n\n\n1.188648\n\n\n\n\n\n\n\n\n\n\n\n\n0.2228084\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000015\n\n\n\n\n\n\n\n\n\n\n\n\n0.0001380\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUrban\n\n\n\n\n\n\n\n\n\n\n\n\nGrade_2012Primary 1\n\n\n\n\n\n\n\n\n\n\n\n\n0.803433\n\n\n\n\n\n\n\n\n\n\n\n\n0.1679609\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000116\n\n\n\n\n\n\n\n\n\n\n\n\n0.0009979\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUrban\n\n\n\n\n\n\n\n\n\n\n\n\nGrade_2013Secondary 1\n\n\n\n\n\n\n\n\n\n\n\n\n-1.497037\n\n\n\n\n\n\n\n\n\n\n\n\n0.4055512\n\n\n\n\n\n\n\n\n\n\n\n\n0.0004833\n\n\n\n\n\n\n\n\n\n\n\n\n0.0401101\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUrban\n\n\n\n\n\n\n\n\n\n\n\n\nGrade_2013Secondary 5\n\n\n\n\n\n\n\n\n\n\n\n\n-1.829716\n\n\n\n\n\n\n\n\n\n\n\n\n0.4934564\n\n\n\n\n\n\n\n\n\n\n\n\n0.0004582\n\n\n\n\n\n\n\n\n\n\n\n\n0.0384918\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUrban\n\n\n\n\n\n\n\n\n\n\n\n\nGrade_2013Primary 2\n\n\n\n\n\n\n\n\n\n\n\n\n-1.922012\n\n\n\n\n\n\n\n\n\n\n\n\n0.3655888\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000020\n\n\n\n\n\n\n\n\n\n\n\n\n0.0001822\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUrban\n\n\n\n\n\n\n\n\n\n\n\n\nGrade_2013Primary 3\n\n\n\n\n\n\n\n\n\n\n\n\n-2.367172\n\n\n\n\n\n\n\n\n\n\n\n\n0.3945070\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000001\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000115\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUrban\n\n\n\n\n\n\n\n\n\n\n\n\nGrade_2013Primary 4\n\n\n\n\n\n\n\n\n\n\n\n\n-2.724553\n\n\n\n\n\n\n\n\n\n\n\n\n0.4261606\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000000\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUrban\n\n\n\n\n\n\n\n\n\n\n\n\nGrade_2013Primary 5\n\n\n\n\n\n\n\n\n\n\n\n\n-2.922021\n\n\n\n\n\n\n\n\n\n\n\n\n0.4293049\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000000\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000005\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUrban\n\n\n\n\n\n\n\n\n\n\n\n\nGrade_2013Primary 6,7,8\n\n\n\n\n\n\n\n\n\n\n\n\n-3.963184\n\n\n\n\n\n\n\n\n\n\n\n\n0.5191857\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000000\n\n\n\n\n\n\n\n\n\n\n\n\n0.0000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe table above as shows a snap short of the analysis of urban areas in Rwanda. As compared to the rural areas, the urban pupil were less likely to repeat, as shown in the table.\n\n\nStrengths and weakness of the data\n\nThe advantages of this data is that;-\n\nThe variables were specific since it was mainly focused in the rural areas.\nThe data was reliable because it did not have many outliers in the variables.\nThe data was observational and designed to control for gender as a cofounding variable\n\n\n\nThe disadvantages of this data was that;-\n\nIt had alot of missing values\n\n\n\nThe variables were highly correlated as shown from some of the test that has been conducted.\nThe data was collected not clean and needed some transformation.\nSome of the variable were not suitable to answer the main object of students repeating or passing."
  },
  {
    "objectID": "posts/02_lift_practical/index.html",
    "href": "posts/02_lift_practical/index.html",
    "title": "Household Survey Practical Analysis blog",
    "section": "",
    "text": "hhh"
  },
  {
    "objectID": "posts/spatial regression - Copy/index.html",
    "href": "posts/spatial regression - Copy/index.html",
    "title": "30 Day Challenge Day1",
    "section": "",
    "text": "Inspired by Georgios KaramanisThis Chart is a contribution to day 1 of the hashtag#30DayChartChallengeHere are facts about the achievement of gender equality reported by www.olympic.com\nYou can get the code here on github 2024 Day 1 chart challenge\n\nCode snippet for Chart:\nlibrary(tidyverse)\nlibrary(ggforce)\nlibrary(camcorder)\n\ngg_record(dir = here::here(\"2024/01/\"), device = \"png\", width = 1080 * 2, height = 1350 * 2, units = \"px\", dpi = 320)\n\n# https://olympics.com/en/news/paris-2024-first-games-to-achieve-full-gender-parity\n\nr &lt;- 1.3\n\n\n\nwomen &lt;- tribble(\n ~olympics, ~year, ~pct, ~x0, ~y0, ~col,\n  \"Tokyo\", 1964, 13, 2*r - r/2, 3*r, \"#0081C8\",\n  \"Tokyo\", 2020, 48.7, 4*r, 3*r, \"black\",\n  \"Beijing\", 2022, 45, 6*r + r/2, 3*r, \"#EE334E\",\n  \"Bueno Aires\", 2018, 50, 3*r - r/4, 2*r, \"#FCB131\",\n  \"Lausanne\", 2020, 50, 5*r + r/4, 2*r, \"#00A651\"\n)\n\nf1 &lt;- \"Graphik\"\nf1b &lt;- \"Graphik Compact\"\nf2 &lt;- \"Produkt\"\nf2b &lt;- \"Produkt Medium\"\n\nggplot(women) +\n  # geom_circle(aes(x0 = x0, y0 = y0, r = 1.3, colour = col), linewidth = 8) +\n  geom_vline(aes(xintercept = x0), alpha = 0.1, linetype = \"dashed\") +\n  geom_arc_bar(aes(x0 = x0, y0 = y0, r = 1.5, r0 = 1.1, start = 0, end = 2 * pi), fill = \"grey99\", color = NA) +\n  geom_arc_bar(aes(x0 = x0, y0 = y0, r = 1.5, r0 = 1.1, fill = col, start = 0, end = pct * 2 * pi / 100), color = NA) +\n  geom_text(aes(x0, y0 + if_else(year %in% c(1932, 1992), -2.5, 2.5), label = paste0(olympics, year, \"\\n\", pct, \"%\"), color = col), lineheight = 0.9, family = f1, size = 5, fontface = \"bold\") +\n  annotate(\"text\", 5.25, 9, label = \"Olympics Gender Parity\\nWomen Equality\", family = f2b, color = \"purple4\", size = 14, lineheight = 0.8) +\n  annotate(\"text\", 5.25, -2, label = paste0(\"Source: www.olympics.com, \", 2023, \"\\n\", \"Graphic: VICTOR MANDELA\"), family = f2, color = \"purple4\") +\n  scale_color_identity() +\n  scale_fill_identity() +\n  coord_fixed(xlim = c(0.5, 10), ylim = c(-2, 10)) +\n  theme_void() +\n  theme(\n    plot.background = element_rect(fill = \"#C9E4DE\", color = NA)\n  )\nOlympic Movement: Achievements in gender equality\n\nParis 1900: Female athletes first took part in the Olympic Games, four years after the first modern Olympics took place in Athens\n1996: Promotion of women becomes a mission of the IOC and is enshrined in the Olympic Charter\nTokyo 2020: The last edition of the Games were the most gender-balanced to date with 48.7 per cent of athletes women. At Tokyo 1964, only 13 per cent of the athletes were women.\nTokyo 2020: Following a rule change allowing one male and one female athlete to jointly carry their flag during the Opening Ceremony, 91 per cent of NOCs had a female flag bearer\nTokyo 2020: Three disciplines achieved gender balance (BMX racing, mountain biking and freestyle wrestling)\n\n\n\n\nBeijing\n\n\n\nBeijing 2022: The last Olympic Winter Games were the most gender balanced to date with 45 per cent female athletes\nParis 2024: Out of the 10,500 athletes participating in the Games, 5,250 will be men and 5,250 women. These Games will be the first to reach full gender parity in terms of number of athletes.\nFemale IOC membership currently stands at 40 per cent, up from 21 per cent at the start of the Olympic Agenda 2020\n\n\n\n\nLausanne\n\n\n\nYouth Olympic Games: The Youth Games Buenos Aires 2018 and Winter Youth Games Lausanne 2020 reached full gender parity in overall athlete participation (2,000 athletes per gender in 2018 and 936 in 2020)\n\n\n\nBueno Aires 2018\n\n\nFemale representation on the IOC Executive Board stands at 33.3 per cent, versus 26.6 per cent before the Olympic Agenda 2020\n50 per cent of the members of IOC Commissions positions have been held by women since 2022, compared with 20.3 per cent prior to the Olympic Agenda 2020. In addition, a record high of 13 of the 31 commissions were chaired by women in 2022."
  },
  {
    "objectID": "posts/Olympics gender equality analysis/index.html",
    "href": "posts/Olympics gender equality analysis/index.html",
    "title": "Comparisons 01: Gender Equality Analysis Olympics",
    "section": "",
    "text": "Inspired by Georgios KaramanisThis Chart is a contribution to day 1 of the hashtag#30DayChartChallengeHere are facts about the achievement of gender equality reported by www.olympic.com\nYou can get the code here on github 2024 Day 1 chart challenge\n\nCode snippet for Chart:\nlibrary(tidyverse)\nlibrary(ggforce)\nlibrary(camcorder)\n\ngg_record(dir = here::here(\"2024/01/\"), device = \"png\", width = 1080 * 2, height = 1350 * 2, units = \"px\", dpi = 320)\n\n# https://olympics.com/en/news/paris-2024-first-games-to-achieve-full-gender-parity\n\nr &lt;- 1.3\n\n\n\nwomen &lt;- tribble(\n ~olympics, ~year, ~pct, ~x0, ~y0, ~col,\n  \"Tokyo\", 1964, 13, 2*r - r/2, 3*r, \"#0081C8\",\n  \"Tokyo\", 2020, 48.7, 4*r, 3*r, \"black\",\n  \"Beijing\", 2022, 45, 6*r + r/2, 3*r, \"#EE334E\",\n  \"Bueno Aires\", 2018, 50, 3*r - r/4, 2*r, \"#FCB131\",\n  \"Lausanne\", 2020, 50, 5*r + r/4, 2*r, \"#00A651\"\n)\n\nf1 &lt;- \"Graphik\"\nf1b &lt;- \"Graphik Compact\"\nf2 &lt;- \"Produkt\"\nf2b &lt;- \"Produkt Medium\"\n\nggplot(women) +\n  # geom_circle(aes(x0 = x0, y0 = y0, r = 1.3, colour = col), linewidth = 8) +\n  geom_vline(aes(xintercept = x0), alpha = 0.1, linetype = \"dashed\") +\n  geom_arc_bar(aes(x0 = x0, y0 = y0, r = 1.5, r0 = 1.1, start = 0, end = 2 * pi), fill = \"grey99\", color = NA) +\n  geom_arc_bar(aes(x0 = x0, y0 = y0, r = 1.5, r0 = 1.1, fill = col, start = 0, end = pct * 2 * pi / 100), color = NA) +\n  geom_text(aes(x0, y0 + if_else(year %in% c(1932, 1992), -2.5, 2.5), label = paste0(olympics, year, \"\\n\", pct, \"%\"), color = col), lineheight = 0.9, family = f1, size = 5, fontface = \"bold\") +\n  annotate(\"text\", 5.25, 9, label = \"Olympics Gender Parity\\nWomen Equality\", family = f2b, color = \"purple4\", size = 14, lineheight = 0.8) +\n  annotate(\"text\", 5.25, -2, label = paste0(\"Source: www.olympics.com, \", 2023, \"\\n\", \"Graphic: VICTOR MANDELA\"), family = f2, color = \"purple4\") +\n  scale_color_identity() +\n  scale_fill_identity() +\n  coord_fixed(xlim = c(0.5, 10), ylim = c(-2, 10)) +\n  theme_void() +\n  theme(\n    plot.background = element_rect(fill = \"#C9E4DE\", color = NA)\n  )\nOlympic Movement: Achievements in gender equality\n\nParis 1900: Female athletes first took part in the Olympic Games, four years after the first modern Olympics took place in Athens\n1996: Promotion of women becomes a mission of the IOC and is enshrined in the Olympic Charter\nTokyo 2020: The last edition of the Games were the most gender-balanced to date with 48.7 per cent of athletes women. At Tokyo 1964, only 13 per cent of the athletes were women.\nTokyo 2020: Following a rule change allowing one male and one female athlete to jointly carry their flag during the Opening Ceremony, 91 per cent of NOCs had a female flag bearer\nTokyo 2020: Three disciplines achieved gender balance (BMX racing, mountain biking and freestyle wrestling)\n\n\n\n\nBeijing\n\n\n\nBeijing 2022: The last Olympic Winter Games were the most gender balanced to date with 45 per cent female athletes\nParis 2024: Out of the 10,500 athletes participating in the Games, 5,250 will be men and 5,250 women. These Games will be the first to reach full gender parity in terms of number of athletes.\nFemale IOC membership currently stands at 40 per cent, up from 21 per cent at the start of the Olympic Agenda 2020\n\n\n\n\nLausanne\n\n\n\nYouth Olympic Games: The Youth Games Buenos Aires 2018 and Winter Youth Games Lausanne 2020 reached full gender parity in overall athlete participation (2,000 athletes per gender in 2018 and 936 in 2020)\n\n\n\nBueno Aires 2018\n\n\nFemale representation on the IOC Executive Board stands at 33.3 per cent, versus 26.6 per cent before the Olympic Agenda 2020\n50 per cent of the members of IOC Commissions positions have been held by women since 2022, compared with 20.3 per cent prior to the Olympic Agenda 2020. In addition, a record high of 13 of the 31 commissions were chaired by women in 2022."
  },
  {
    "objectID": "posts/Kenya trade on milk/index.html",
    "href": "posts/Kenya trade on milk/index.html",
    "title": "Kenya Milk Export and Imports Analysis with R",
    "section": "",
    "text": "Election periods cause tension and affect agricultural production, in 2017 it led to a sharp decrease in Milk supply and caused an import rise of the commodity for a period.\n\n\n\nHarvard: Picture of People Casting Votes\n\n\nHowever, during the outbreak of covid19 Kenya experienced import restrictions causing a slight decrease.\nSource of Data is FAOSTATS\n\n\n\nPicture: Pinterest\n\n\nHear is a simple step-wise analysis of generating the code:-\nlibrary(tidyverse)\nlibrary(here)\nlibrary(ggtext)\n# remotes::install_github(\"AllanCameron/geomtextpath\")\nlibrary(geomtextpath)\nlibrary(camcorder)\n\n\ngg_record(dir = here::here(\"2024/02\"), device = \"png\", width = 1080 * 2, height = 1350 * 2, units = \"px\", dpi = 320)\n\n\n\n# source: https://www.fao.org/faostat/en/#data/TCL\n\n\n# width to height\naspect_ratio &lt;- 1280 / 946\n\n\n\ndf &lt;- read_csv(\"2024/data/kenya_milk_trade_2006-2022.csv\") %&gt;% \n  select(year = Year,\n         Element, Value) %&gt;% \n  pivot_wider(names_from =  \"Element\",\n              values_from = \"Value\") %&gt;% #, names_to = c(\"Export\", \"Import\")\n  rename(imports = \"Import Quantity\",\n          exports = \"Export Quantity\") %&gt;% \n  mutate(imports = round(imports,0)/10000,\n         exports = round(exports, 0)/10000)\n\n# apply lowess smoothing to the import / export data \nsmooth_bw &lt;- 0.17\n\napply_smoothing &lt;- function(x, y, bw) {\n  smooth &lt;- lowess(x, y, f = smooth_bw)\n  smooth$y\n}\n\ndf$imports_smooth &lt;- apply_smoothing(df$year, df$imports, bw = smooth_bw)\ndf$exports_smooth &lt;- apply_smoothing(df$year, df$exports, bw = smooth_bw)\n\n\nbase_font_family &lt;- \"Abhaya Libre\"\nline_color &lt;- \"#ABA098\"\nfont_color &lt;- \"#564D46\" # \"#45403E\"\n\n\n\naxis_labels_y &lt;- as.character(seq(1, 15 , 1))\naxis_labels_y[axis_labels_y == \"100\"] &lt;- \"100,000\"\n\ngeom_textline2 &lt;- function(..., stat = \"unique\", linecolor = NA, \n                           color = \"#554C49\", family = base_font_family, \n                           fontface = \"bold\", size = 3) {\n  geom_textline(...,\n                stat = stat, linecolor = linecolor, color = color, alpha = 0.87,\n                family = family, size = size)\n}\n\n\n# png(here(base_path, \"03-historical.png\"), res = 300, units = \"in\",\n    # width = 6, height = 6 / aspect_ratio)\np &lt;- df %&gt;% \n  ggplot2::ggplot(aes(year)) +\n  geom_hline(yintercept = 100, size = 1, col = \"#ABA098\") +\n  \n  # Highlighted area\n  geom_ribbon(\n    data = . %&gt;% filter(year &lt;= 2017),\n    aes(x = year, ymin = exports_smooth, ymax = imports_smooth),\n    fill = \"#F2DBD8\", alpha = 0.9) +\n  geom_ribbon(\n    data = . %&gt;% filter(year &gt;= 2017),\n    aes(x = year, ymin = exports_smooth, ymax = imports_smooth),\n    fill = \"#E5D9B9\", alpha = 0.9) +\n  \n  # smoothed lines (thick coloured lines + thin grey lines)\n  geom_smooth(aes(y = exports),\n              size = 1.5, se = FALSE, span = 0.25, alpha = 0.8,\n              col = \"#A3555B\", method = \"loess\") +\n  geom_smooth(aes(y = imports),\n              size = 1.5, se = FALSE, span = 0.25, alpha = 0.8,\n              col = \"#D8A962\", method = \"loess\") +\n  geom_smooth(aes(y = exports),\n              size = 0.2, se = FALSE, span = 0.28, alpha = 0.2,\n              col = \"grey40\", method = \"loess\") +\n  geom_smooth(aes(y = imports),\n              size = 0.2, se = FALSE, span = 0.28, alpha = 0.2,\n              col = \"grey40\", method = \"loess\") +\n  \n  # Annotations\n  geom_textline2(aes(y = imports, label = \"Line of Imports\"),\n                 vjust = 0, hjust = 0.3) +\n  geom_textline2(aes(y = exports, label = \"Line of Exports\"),\n                 vjust = 1, hjust = 0.3) +\n  geom_textline2(aes(y = imports, label = \"Imports\"),\n                 vjust = 1, hjust = 0.95) +\n  geom_textline2(aes(y = exports, label = \"Exports\"),\n                 vjust = 0, hjust = 0.85) +\n  \n  # geom_textline2(aes(y = exports, label = \"BALANCE AGAINST\"),\n  #                vjust = -1.5, hjust = 0.38, size = 3.5, \n  #                family = \"Taviraj Bold Italic\", \n  #                # family = \"Old Standard TT\", fontface = \"bold\"\n  # ) +\n  annotate(\"richtext\", x = 2019, y = 5, \n           label = \"Post Election&lt;/span&gt; \n           &lt;i style='font-family: Taviraj Italic; font-size: 7pt'&gt;in&lt;/i&gt; &lt;br&gt;\n           2017\n           &lt;i style='font-family: Taviraj Italic; font-size: 7pt'&gt;and&lt;/i&gt; &lt;br&gt;\n           COVID19.\", \n           size = 4, family = \"Taviraj Bold Italic\", \n           hjust = 0, vjust = 1,  color = \"#554C49\", label.size = 0, fill = NA) +\n  \n  scale_x_continuous(breaks = seq(2006, 2022, 2)) +\n  scale_y_continuous(position = \"right\", breaks = seq(1, 15, 1),\n                     labels = axis_labels_y) +\n  coord_cartesian(ylim = c(0, 15), expand = FALSE, clip = \"off\") +\n  labs(\n    title = \"Exports and Imports of Milk in \n    &lt;span style='font-size: 14pt'&gt;Kenya&lt;/span&gt; from 2006 to 2022.\",\n    caption = paste(\"The Bottom line is divided into Years,\n    the Right hand line into L10,000 each.\",\n                    \"&lt;br&gt;&lt;span style='font-size:4pt'&gt;Designed for 30DayChartChallenge, 02&lt;sup&gt;t&lt;/sup&gt; April 2024, by Victor Mandela\",\n                    \"&lt;span style='color:transparent'&gt;\",\n                    paste(rep(\".\", 200), collapse = \"\"),\n                    \"&lt;/span&gt;\",\n                    \"Nairobi, 047, CBD, Kenya&lt;/span&gt;\")) +\n  theme_minimal(base_family = base_font_family, base_size = 9) +\n  theme(\n    plot.background = element_rect(color = NA, fill = \"#FEFEFF\"),\n    axis.title = element_blank(),\n    axis.text = element_text(face = \"bold\"),\n    panel.grid.major = element_line(color = line_color, size = 0.3),\n    panel.grid.minor = element_blank(),\n    text = element_text(color = font_color),\n    plot.title = element_markdown(face = \"bold\", hjust = 0.5),\n    plot.caption = element_markdown(hjust = 0.6, size = 9, lineheight = 0.8,\n                                    family = \"Charm\", face = \"bold\",\n                                    margin = margin(t = 8)),\n    plot.margin = margin(t = 16, b = 8, l = 12, r = 12))\np\ninvisible(dev.off())\n\n\n# Add the border around the plot area - fair play to William Playfair\nlibrary(grid)\n\npng(here(base_path, \"03-historical-wframe.png\"), res = 300, units = \"in\",\n    width = 6, height = 6 / aspect_ratio)\np + annotation_custom(\n  rectGrob(gp = gpar(col = \"#4B4543\", fill = NA, lwd = 3)),\n  xmin = 1699.5, xmax = 1787, ymin = -2, ymax = 202\n) + annotation_custom(\n  rectGrob(gp = gpar(col = \"#4B4543\", fill = NA, lwd = 0.5)),\n  xmin = 1700, xmax = 1786.5, ymin = 0, ymax = 200\n)\n\nCredit to Ansgar Wolsing."
  },
  {
    "objectID": "posts/New HIV infections among Children/index.html",
    "href": "posts/New HIV infections among Children/index.html",
    "title": "Progress and Challenges: HIV Infections in Kenya from 2019 to 2021 Analysis with R",
    "section": "",
    "text": "Introduction\n\nIn recent years, Kenya has made significant strides in combating HIV/AIDS, but challenges remain. Understanding trends in new infections is crucial for guiding interventions and policy decisions. This article examines the trends in new HIV infections in Kenya from 2019 to 2021, focusing on both overall and pediatric cases.\n\n\n\nMap of Kenya\n\n\nOverview of New HIV Infections: According to data from the National AIDS Control Council (NACC) of Kenya, the total number of new HIV infections in the country declined slightly from 53,200 in 2019 to 52,200 in 2021. While this represents a modest decrease of 1.13%, it signifies progress in the fight against the epidemic.\nTrends Among Children: One particularly encouraging trend is the decrease in new HIV infections among children aged 0 to 14. In 2019, there were 6,200 new pediatric infections, which decreased to 5,200 in 2021, marking a notable 16.13% reduction over the two-year period.\n\nFactors Contributing to the Decline: Several factors may have contributed to the decline in new HIV infections in Kenya. Expanded access to HIV testing and counseling services, improved maternal and child health programs, and the scale-up of prevention of mother-to-child transmission (PMTCT) interventions have likely played a role. Additionally, increased awareness, education, and community mobilization efforts have helped reduce stigma and discrimination associated with HIV/AIDS, encouraging more people to seek testing and treatment.\nChallenges and Areas for Improvement: Despite the progress made, challenges persist in the fight against HIV/AIDS in Kenya. Access to comprehensive prevention services, including condoms and pre-exposure prophylaxis (PrEP), remains uneven, particularly among key populations such as sex workers, men who have sex with men, and people who inject drugs. Additionally, gaps in testing coverage and linkage to care continue to hinder efforts to achieve epidemic control.\n\nConclusion: In conclusion, the modest decrease in new HIV infections overall and the significant reduction among children aged 0 to 14 in Kenya from 2019 to 2021 is a testament to the country’s commitment to ending the epidemic. However, sustained efforts are needed to address remaining challenges and achieve the goal of an AIDS-free generation. By continuing to prioritize evidence-based interventions, invest in health systems strengthening, and promote equity and inclusivity, Kenya can build on its progress and move closer to ending HIV/AIDS.\nSources:\n\nNational AIDS Control Council (NACC) Kenya. (2022). Kenya AIDS Strategic Framework 2021/22 - 2025/26. Retrieved from https://nacc.or.ke\nKenya Ministry of Health. (2021). Kenya HIV Estimates Report 2021. Retrieved from https://www.health.go.ke\n\nAttached is the r code used to generate.\n## packages\nlibrary(tidyverse)\nlibrary(ggtext)\nlibrary(ggrepel)\nlibrary(patchwork)\nlibrary(systemfonts)\nlibrary(camcorder)\n\n\ngg_record(dir = here::here(\"2024/03\"), device = \"png\", \n          width = 1500 * 2, height = 1350 * 2, units = \"px\", dpi = 320)\n\n\ntheme_set(theme_minimal(base_size = 19, base_family = \"Girassol\"))\n\ntheme_update(\n  text = element_text(color = \"grey12\"),\n  axis.title = element_blank(),\n  axis.text.x = element_text(family = \"Iosevka Curly\"),\n  axis.text.y = element_blank(),\n  panel.grid.major.y = element_blank(),\n  panel.grid.minor = element_blank(),\n  plot.margin = margin(20, 5, 10, 10),\n  plot.subtitle = element_textbox_simple(family = \"Roboto Condensed\", size = 14,\n                                         lineheight = 1.6),\n  plot.title.position = \"plot\",\n  plot.caption = element_text(family = \"Iosevka Curly\", color = \"#b40059\", hjust = .5,\n                              size = 10, margin = margin(35, 0, 0, 0))\n)\n\n# read the data\ndf &lt;- read_csv(\"2024/data/new_infected_data.csv\")\n\n#prep data\ndf_prep &lt;- df %&gt;%\n  pivot_longer(cols = c(-Newly_infected_with_HIV)) %&gt;% \n  rename(year = name, \n         n = value) %&gt;% \n  group_by(Newly_infected_with_HIV) %&gt;% \n  mutate(\n    total = sum(n),\n    current = n[which(year == 2021)]\n  ) %&gt;% \n  ungroup() %&gt;%\n  mutate(\n    Newly_infected_with_HIV = fct_reorder(Newly_infected_with_HIV, total),\n    Newly_infected_with_HIV = fct_relevel(Newly_infected_with_HIV, \"Young_people_(ages 15-24)\")\n  )\n\n\n# summary computation\ndf_sum &lt;-\n  df_prep %&gt;% \n  filter(year &lt;= 2021) %&gt;% \n  group_by(year) %&gt;% \n  summarize(n = sum(n))\n\n# plot\np1 &lt;- \n  df_sum %&gt;% \n  ggplot(aes(year, n)) +\n  geom_col(aes(fill = factor(year)), width = .85) +\n  geom_col(\n    data = df_prep %&gt;% filter(Newly_infected_with_HIV == \"Children_(ages 0-14)\" & year &lt;= 2021),\n    aes(alpha = year == 2021),\n    fill = \"blue\", width = .5\n  ) +\n  geom_text(\n    data = df_sum %&gt;% \n      mutate(n_lab = if_else(year %in% c(2012, 2021), paste0(n, \"\\nTotal\"), as.character(n))),\n    aes(label = n_lab), \n    family = \"Iosevka Curly\", size = 3.3, lineheight = .8, \n    nudge_y = 12, vjust = 0, color = \"black\", fontface = \"bold\"\n  ) +\n  geom_text(\n    data = df_prep %&gt;% filter(Newly_infected_with_HIV == \"Children_(ages 0-14)\" & year &lt;= 2021) %&gt;% \n      mutate(n_lab = if_else(year %in% c(2012, 2021), paste0(n, \"\\nChildren\"), as.character(n))), \n    aes(label = n_lab), \n    family = \"Iosevka Curly\",\n    color = \"white\", lineheight = .8, size = 3.0, \n    nudge_y = 12, vjust = 0, fontface = \"bold\"\n  ) +\n  geom_text(\n    data = df_prep %&gt;% filter(Newly_infected_with_HIV == \"Children_(ages 0-14)\" & year &lt;= 2021),\n    aes(y = -15, label = year, color = factor(year)), \n    family = \"Iosevka Curly\", size = 6, hjust = .5, vjust = 1\n  ) +\n  coord_cartesian(clip = \"off\") +\n  scale_y_continuous(limits = c(-15, NA)) +\n  scale_color_manual(values = c(rep(\"black\", 9), \"#b40059\", \"grey70\"), guide = \"none\") +\n  scale_fill_manual(values = c(rep(\"purple\", 9), \"#b40059\", \"yellow\"), guide = \"none\") +\n  scale_alpha_manual(values = c(.25, .4), guide = \"none\") +\n  labs(title = \"&lt;br&gt;&lt;span style='font-size:20pt'&gt;New HIV infections among children in Kenya decreased by 13% in the &lt;b style='color:#b40059'&gt;Pandemic Year&lt;/b&gt;\",\n       subtitle = \"&lt;br&gt;&lt;span style='font-size:14pt'&gt;From 2019 to 2021, there was a 1.13% decrease in new HIV infections overall in Kenya, and a 16.13% decrease in new infections among children aged 0 to 14.\",\n       caption = paste(\"&lt;br&gt;&lt;span style='font-size:6pt'&gt;&lt;b style='color:black'&gt;Source: World Bank  \", \"      \", \"\\nGraphics: Victor Mandela\")) +\n  theme(\n    plot.title = element_markdown(size = 28, margin = margin(5, 35, 25, 35), color = \"black\"),\n    plot.subtitle = element_textbox_simple(margin = margin(5, 35, 15, 35)),\n    panel.grid.major = element_blank(),\n    axis.text.x = element_blank(),\n    plot.caption = element_markdown(hjust = 0.6, size = 9, lineheight = 0.8,\n                                    family = \"Charm\", face = \"bold\",\n                                    margin = margin(t = 8))\n  )\n\np1"
  },
  {
    "objectID": "posts/KDHS 101 Converting Column header data/index.html",
    "href": "posts/KDHS 101 Converting Column header data/index.html",
    "title": "DHS data - Convert Column Values into Column Headers in R",
    "section": "",
    "text": "Why I Haven’t Posted in a While... But I’m Back!\nHello, wonderful readers! I know, I know, it’s been a while since my last post. You might have been wondering, “Did they get lost in a sea of datasets? Or perhaps they took up fishing full-time?” Well, I did catch some impressive fish, but I’m happy to report that I haven’t abandoned my love for all things data. Life got busy, projects piled up, and before I knew it, my blog started to gather digital dust. But worry not, I’m back with some exciting content to share with you!\n\nToday, I want to dive into the fascinating world of the Demographic and Health Surveys (DHS) data. If you’ve ever worked with this dataset, you’ll know it’s a treasure trove of information that can tell you everything from health indicators to demographic trends across various countries. This data is incredibly important for researchers, policymakers, and anyone interested in understanding population health and demographic patterns. However, if you’ve ever tried to make sense of those cryptic column headers, you might have felt like you were decoding an ancient script. Fear not! I’m here to guide you through converting those headers into something more human-friendly and to show you how to pattern search the names with ease.\n\nWhat is DHS Data, and Why Should You Care?\nThe DHS program provides data that’s crucial for decision-making in health policy and planning. These surveys cover a wide range of topics, including fertility, maternal and child health, nutrition, and much more. They’re used by governments, NGOs, and researchers worldwide to monitor and evaluate health programs, develop policies, and conduct research. In short, if you’re involved in any kind of health or demographic research, DHS data is like the holy grail.\n\n\nThe Mystery of the Cryptic Column Headers\nOne of the biggest challenges when working with DHS data is the column headers. Instead of straightforward names like “Age” or “Number of Children,” you get something like “V012” and “V034.” It’s like trying to read your cat’s mind. But don’t worry; with a few tricks, you can convert these mysterious codes into meaningful names.\n\n\n\nConverting Values in Headers to Actual Names\nFirst things first, you need to get your hands on the DHS recode manual, which is essentially the Rosetta Stone for these datasets. This manual will help you translate those enigmatic codes into understandable variable names using R. Here’s a step-by-step guide:\n#First load the data and package\nlibrary(haven) # for loading .dta data\nlibrary(tidyverse) #for manipulation\nlibrary(labelled) #for accessing the labels\nlibrary(purrr) #for parrallet computation\nlibrary(data.table)\nlibrary(janitor)\n\n# reading in sav data -- women data\ndf1 &lt;- haven::read_dta(\"DT/KEBR8BFL.dta\")\n\n \n# Define the harvest function\n# This function is for converting names from the stata value format\n# to the column heads for easy searching\n\nrename_fun &lt;- function(df){\n  var_labels &lt;- var_label(df)\n  new_names &lt;- purrr::map_chr(df, ~ attr(.x, \"label\"))\n  new_names[is.na(new_names)] &lt;- names(df)[is.na(new_names)] # Retain original names for columns without labels\n  names(df) &lt;- new_names\n  data_df &lt;- as.data.table(df)\n  return(data_df)\n}\n\n# clean the names\ndf1 &lt;- rename_fun(df1) %&gt;% \n  janitor::clean_names()\n\n# Find the names with a pattern format using data.tables's %like% \nnames_df1 &lt;- names(df1)[names(df1) %like% \"residence\"]\nprint(names_df1)\n\n# to get the values - use print_labels() from the labelled packega\nprint_labels(names_df1$religion)\n\n\n\nWorking with DHS data can be daunting at first, but with the right approach, you’ll soon find yourself navigating it like a pro. And remember, data is only as good as your ability to understand and use it effectively. So take the time to decode those headers and make your analysis more meaningful."
  },
  {
    "objectID": "posts/KDHS Survey Design/index.html",
    "href": "posts/KDHS Survey Design/index.html",
    "title": "DHS Survey Design Computation",
    "section": "",
    "text": "When working with Demographic and Health Surveys (DHS) data, it is crucial to understand certain standardized variables that are consistent across different countries and survey rounds. Three of these essential variables are v005, v021, and v022. These variables play a vital role in ensuring accurate and representative analysis of DHS data.\n\n\n\nThe v005 variable represents the sample weight for each individual in the survey. It is a six-digit number with 6 implied decimal places. Sample weights are used to adjust for the probability of selection, non-response, and other adjustments to ensure that the survey results are representative of the entire population. When analyzing DHS data, it is crucial to use these sample weights to obtain unbiased and accurate estimates.\n\n\n\n\nThe v021 variable indicates the primary sampling unit or cluster number. In DHS surveys, households are often grouped into clusters known as PSUs. This variable helps in accounting for the survey's complex design by identifying these clusters. Properly accounting for PSUs is essential for accurate variance estimation and analysis.\n\n\n\n\nThe v022 variable represents the sample stratum number. Stratification is a technique used in survey sampling to divide the population into different subgroups, or strata, based on certain characteristics. In DHS surveys, strata are often formed by geographic regions and urban/rural areas. This variable is important for specifying the stratification in the survey design, which is critical for proper weighting and variance estimation.\n\n\n\nTo effectively analyze DHS data, it is important to account for these variables in your statistical analysis. Here is an example of how to use these variables in R to set up\n# Install and load the survey package\ninstall.packages(\"survey\")\nlibrary(survey)\n\n# Assuming your DHS data is in a data frame called df\n# Create a survey design object\ndhs_design &lt;- svydesign(\n  id = ~v021,        # Primary Sampling Unit (PSU)\n  strata = ~v022,    # Strata\n  weights = ~v005,   # Sample weights\n  data = df,\n  nest = TRUE\n)\n\n# Now you can use the dhs_design object to perform weighted analyses\n\nThis setup allows you to correctly analyze the DHS data, taking into account the complex survey design, including clustering, stratification, and sampling weights.\n\n\n\nUnderstanding the roles of v005, v021, and v022 in DHS data is essential for accurate and representative analysis. By properly incorporating these variables into your analysis, you can ensure that your findings are both valid and reliable. Whether you are a seasoned researcher or new to DHS data, mastering these key variables will significantly enhance the quality of your analysis."
  },
  {
    "objectID": "posts/KDHS Survey Design/index.html#understanding-key-variables-in-dhs-data-v005-v021-and-v022",
    "href": "posts/KDHS Survey Design/index.html#understanding-key-variables-in-dhs-data-v005-v021-and-v022",
    "title": "DHS Survey Design Computation",
    "section": "",
    "text": "When working with Demographic and Health Surveys (DHS) data, it is crucial to understand certain standardized variables that are consistent across different countries and survey rounds. Three of these essential variables are v005, v021, and v022. These variables play a vital role in ensuring accurate and representative analysis of DHS data.\n\n\n\nThe v005 variable represents the sample weight for each individual in the survey. It is a six-digit number with 6 implied decimal places. Sample weights are used to adjust for the probability of selection, non-response, and other adjustments to ensure that the survey results are representative of the entire population. When analyzing DHS data, it is crucial to use these sample weights to obtain unbiased and accurate estimates.\n\n\n\n\nThe v021 variable indicates the primary sampling unit or cluster number. In DHS surveys, households are often grouped into clusters known as PSUs. This variable helps in accounting for the survey's complex design by identifying these clusters. Properly accounting for PSUs is essential for accurate variance estimation and analysis.\n\n\n\n\nThe v022 variable represents the sample stratum number. Stratification is a technique used in survey sampling to divide the population into different subgroups, or strata, based on certain characteristics. In DHS surveys, strata are often formed by geographic regions and urban/rural areas. This variable is important for specifying the stratification in the survey design, which is critical for proper weighting and variance estimation.\n\n\n\nTo effectively analyze DHS data, it is important to account for these variables in your statistical analysis. Here is an example of how to use these variables in R to set up\n# Install and load the survey package\ninstall.packages(\"survey\")\nlibrary(survey)\n\n# Assuming your DHS data is in a data frame called df\n# Create a survey design object\ndhs_design &lt;- svydesign(\n  id = ~v021,        # Primary Sampling Unit (PSU)\n  strata = ~v022,    # Strata\n  weights = ~v005,   # Sample weights\n  data = df,\n  nest = TRUE\n)\n\n# Now you can use the dhs_design object to perform weighted analyses\n\nThis setup allows you to correctly analyze the DHS data, taking into account the complex survey design, including clustering, stratification, and sampling weights.\n\n\n\nUnderstanding the roles of v005, v021, and v022 in DHS data is essential for accurate and representative analysis. By properly incorporating these variables into your analysis, you can ensure that your findings are both valid and reliable. Whether you are a seasoned researcher or new to DHS data, mastering these key variables will significantly enhance the quality of your analysis."
  },
  {
    "objectID": "posts/Skills to excel and word/index.html",
    "href": "posts/Skills to excel and word/index.html",
    "title": "Received the Job, Excel and Word for Life!",
    "section": "",
    "text": "🚨 Beware of Innovation Black Holes: My Experience with Stifled Skills 🚨\n\n\nHave you ever landed your dream job, only to find yourself trapped in a time warp where your cutting-edge skills are rendered useless? Imagine joining a company, eager to apply your advanced techniques, only to spend over a year battling with Excel, Word, and PowerPoint. 📊📄📈\n\n\n\nYes, you read that right. I found myself in a professional paradox. My excitement to innovate was met with a management style that was more “status quo” than “let’s grow.” 🚫💡\n\n\n\nThe Reality Check\nIf a workplace doesn’t encourage new ideas or innovation, it’s a red flag 🚩. You might be in a toxic environment where your potential is not just underutilized, but entirely ignored.\n\n\n\n\n\n🔍 Do Your Homework\nBefore jumping ship to a new role, especially with big companies or NGOs, dig deep. Look beyond the shiny job description and ask the tough questions:\n\nHow does the company support innovation?\nAre there opportunities for professional growth?\nWhat’s the management’s attitude towards new ideas?\n\n\n\n✨ Career Motivators, Listen Up\nYour skills are your greatest asset. Don’t let them collect dust in a place that doesn’t value progress. Advocate for yourself, and if you find yourself in a stifling environment, it might be time to move on. 🌟\n\n\nTo all job seekers out there: trust your gut and do your due diligence. The right company will not only recognize your talents but will also provide a fertile ground for your growth and innovation. 🌱\n\n\n\n\n\n\nConclusion\nRemember, your career is too precious to waste in an innovation black hole. Stay curious, stay ambitious, and never settle for less than what you deserve. 💪🚀"
  },
  {
    "objectID": "posts/Team morale/index.html",
    "href": "posts/Team morale/index.html",
    "title": "Boosting Team Performance",
    "section": "",
    "text": "🚀 Boosting Team Performance: Lessons Learned from My Journey in Monitoring & Evaluation 🌟\n\n\nIn the fast-paced world of Monitoring and Evaluation (M&E), staying ahead requires constant innovation and dedication. After more than six years of navigating this field, making countless mistakes, and learning the hard way, I've finally honed some key insights that have significantly improved team performance and project outcomes. Here's what I've learned and want to share with you in less than 2 minutes.\n\n\n📚 Comprehensive Training Materials & Protocols: A Game Changer\nOne of the major milestones in my career has been the development of comprehensive training materials and protocols. After many trial and error moments, the results finally showed—a 40% increase in staff proficiency and adherence to established procedures! This improvement not only streamlined our workflow but also enhanced the quality of our outputs. 🎯\n\n\n\n\n\n🛠️ Leading Capacity Building: Empowering Through Knowledge\nIn my early years, I underestimated the power of proper capacity building. But leading efforts in drafting research methodology, sampling designs, research design, and creating CAPI questionnaires has been transformative. This process has empowered our team to deploy these tools effectively, ensuring that we maintain high standards in our research activities. 💡\n\n\n\n\n\n🔍 Coordinating Data Collection & Analysis: From Numbers to Narratives\nCoordinating data collection, analysis, interpretation, and presentation of research and evaluation results used to be a daunting task. I made many mistakes along the way. However, with persistence and learning, I now transform raw data into actionable insights that drive decision-making and strategic planning. 📊\n\n\n\n\n\n🤝 Cross-Departmental Collaboration: Enhancing Synergy\nSupporting other departments through monitoring, research, statistical analysis, and implementation meetings has taught me the value of collaboration. Early on, I often worked in silos, missing out on valuable insights. Now, these brainstorming sessions have led to innovative solutions and enhanced project outcomes. 🌐\n\n\n\n\n\n🎯 Executing Research Projects: Measuring Impact\nExecuting and implementing research projects that systematically investigate the impact of our platform was another area where I faced challenges. By embracing quasi-experimental or experimental study designs, we've now been able to provide robust evidence of our platform's effectiveness. This scientific approach ensures that our findings are reliable and actionable. 🧪\n\n\n\n\n\n\n🌟 Takeaway: Innovation and Collaboration are Key\nThe key to success in M&E lies in continuous learning and collaboration. By developing effective training protocols, leading capacity building, and fostering cross-departmental synergy, we can achieve significant improvements in performance and outcomes.\n🔗 What are your thoughts? Share your experiences or insights in the comments below! Let's keep the conversation going. 💬"
  },
  {
    "objectID": "posts/Creating a profile Website/index.html",
    "href": "posts/Creating a profile Website/index.html",
    "title": "Creating a website for free: Ultimate guide",
    "section": "",
    "text": "Tools needed\nNote that you don’t have to have expert programming for this course.\nIn preparation we need the following:-\nR - https://cran.r-project.org/bin/windows/base/\nRstudio - https://rstudio.com/products/rstudio/download/\ngit for desktop- https://git-scm.com/downloads\nAlso, you need to create an account with:- \nnetlify - https://www.netlify.com/\ngithub - https://github.com/\nSetting up R and R studio(positron) or any other IDE\n\nInstall the R and R studio.\nAlternative you can install Pycharm, VSCode\nAlternatively you can use Jupiter Notebook\n\n\nConnect rstudio to github account\n\nGo to rstudio &gt; tools &gt; Global options &gt;  GIT/SVN  &gt; SSH RSA key &gt; Create RSA key &gt; view public key &gt; copy\nGo to github &gt; Settings (top right corner) &gt; SSH and GPG keys &gt; SSH keys &gt;  New SSH key &gt; title (put your name) &gt; key (paste the public key) &gt; add SSH key\n\nSet up the git for desktop\n\ngit config –global user.name ‘your github name’\ngit config –global user.email ‘email address you used on github’\ngit config –global –list\n\nCreate repository and clone it on github\n\nOpen your github account and create new repository\nClone the repository\n\n\nPicture Source: FAOStats"
  },
  {
    "objectID": "Articles and Journals.html",
    "href": "Articles and Journals.html",
    "title": "Publications",
    "section": "",
    "text": "Practices, Board functions and performance of Devolved units in Kenya IJECM.com.uk vol. 10. No 4 Find the journal here\n\n\n\n\nInfluence of Job Redesign on Employee commitment among Survivors of restructuring in state corporation in Kenya IJSSE volume.6. no. 5. 2016 Find link here\nInfluence of Board functions on Performance of Kenyan County Governments IJSSE. Volume6. no.4. 2016 Link Here\n\nGovernance structures Board functions and Performance of Devolved units in Kenya Volume 10. No. 3. (2020) DBA Link Here\n\nBoards Accountability practices, Governance Structures and performance of country Governments in Kenya Uon library Repository 2020 6, Role of Audit Committee in Promoting Accountability in Constituency Find Link Here\n\nDevelopment Fund in Nairobi County Uon library Repository 2010 6. 6.\nElectronic trading, investment, market efficiency and performance in east africa security exchanges\nAccountability Practices, Board Function and Performance of County Governments in Kenya;\nEffect of forensic accountants in strengthening internal control of county governments; a study of selected county governments in Kenya; KCA University, Nairobi Kenya."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Practices, Board functions and performance of Devolved units in Kenya IJECM.com.uk vol. 10. No 4 Find the journal here\n\n\n\nInfluence of Job Redesign on Employee commitment among Survivors of restructuring in state corporation in Kenya IJSSE volume.6. no. 5. 2016 Find link here\n\ninfluence of Board functions on Performance of Kenyan County Governments IJSSE. Volume6. no.4. 2016\nGovernance structures Board functions and Performance of Devolved units in Kenya Volume 10. No. 3. (2020) DBA\nBoards Accountability practices, Governance Structures and performance of country Governments in Kenya Uon library Repository 2020 6, Role of Audit Committee in Promoting Accountability in Constituency\nDevelopment Fund in Nairobi County Uon library Repository 2010 6. 6.\nElectronic trading, investment, market efficiency and performance in east africa security exchanges\nAccountability Practices, Board Function and Performance of County Governments in Kenya;\nEffect of forensic accountants in strengthening internal control of county governments; a study of selected county governments in Kenya; KCA University, Nairobi Kenya."
  },
  {
    "objectID": "Supervisions.html",
    "href": "Supervisions.html",
    "title": "Testimonies and Supervisions",
    "section": "",
    "text": "SUPERVISORY MASTERS STUDENTS RESEARCH\n\nEffect of Forensic Accounting Techniques on Fraud Prevention in Deposit-Taking Savings and Credit Cooperative Organizations in Nairobi, Kenya; Mokaya Gladys Kerubo ondari; Reg No: 17/00675\nEffect of Audit Committee Characteristics on Financial Performance of Savings and Credit Cooperative Societies in Kenya; Chebor K. Christopher; Adm No 19/01420\nEffect of Risk Management Practices on Financial Performance of Development Financial institutions in Kenya; Chacha Mwita Marwa; Reg: 19/07031; Kca University A Master of Science Degree in Development Finance\nEffect of Microfinance Services on ohe Growth of Women-Owned Enterprises in Kiambu County; by Esther N. Kinga; Master of Science (Finance and Accounting Option) School of Business Kca), University\nEffect of Forensic Accounting on Financial Performance of Nairobi Securities Exchange Listed insurance Firms Kenya; Kobi Matib; Master of Science in Commerce; Finance and Accounting; Kca University; 20235.\nEffect of External Auditing Practices on Financial Accountability in the County Government of Kitui Kenya; onesmus Kitonyo; No: 18/02763; Master of Science in Commerce (Finance and Accounting; Kca University\nEffect of Microfinance Services on the Growth of Women-Owned Enterprises in Kiambu County; Esther N. Kinga; No: 22/00694; Master of Science (Finance and\nAccounting Optionkca, University; July, 2023\n\nEXAMINER- MASTERS PROPOSAL\n\n“Impact of Basel Accord on Risk Management by Listed Commercial Banks in Kenya”; Reg No: Adm: 13/00809; Omuse Duncan\nEffect of Financial Literacy on Rental Tax Compliance Among Landlords in Machakos County Kenya; Elias Aguta Mochiemo; Reg. No. Kca19/00892; Master of Finance and Accounting; Kca University; 2023\nEffect of Financial Risks on Financial Performance of Deposit-Taking Savings and Credit Cooperative Societies in Kiambu County, Kenya; Geoffrey Mweu Kithuka; Admn No: 10/01614; Master of Finance and Accounting; Kca University; 2023\n“Development Finance and Economic Development in Kenya” Grace Kanini Mbuta\n“Impact of Basel Accord on Risk Management by Listed Commercial Banks in Kenya” Omuse Duncan; 13/00809;\n“Effect of Forensic Accounting on Financial Performance of County Governments of Kenya” Timothy Omonge; 22/02631\n“Effect of Bank Characteristics on interest Rate Spread of Commercial Banks in Kenya; Nelly Akoth Mitoko\nthe Effects of Financing Strategies on the Growth of the Real Estate Sector in Nairobi Metrpolitan in Kenya; Silas Wanjala Osundwa; 12/02681;; Masters ; Kca University; October, 2023\n\nSUPERVISORY/EXAMINER DEGREE STUDENTS RESEARCH\n\nThe Impact of Accounting information Systems Usage on Financial Performance of Manufacturing Companies in Nairobi; Mutisya Esther Kalekye; 19/02222; Bachelot of Commerce Degree Kca Universitty; 2023\nThe Effect of Training and Development on Employee Performance in Manufacturing Companies in Kenya; A Case Study of Epza Kenya; Priscah Jane Kasina; 21/00368; Bachelor of Commerce (Bcom) Human Resource Option in Kca University; May 2023\nFactors Affecting the integrity of Financial Statements in the Public Sector and Performance; (A Case Study of Public institutions in Kiambu County); Njuguna Ruth Magiri; Reg No. 21/01233; Bachelors of Commerce Degree Programme; Kca University December 2022\nEffect of Planning on Purchasing and Performance in Manufacturing Companies in Kenya: A Case Study of Athi-River Limited Company; Nzioka Gerald Manasseh;\n19/03192; Bachelor of Procurement and Logistics in Kca University; November 2022\nThe Effect of Mergers and Acquisitions on the Financial Performance of Commercial Banks; Kamau Joseph Wanyoike; Kcau/11/00947; Bachelor of Commerce Degree; April 2023\n\nMANUALS AND BOOKS FOR PUBLICATIONS\n\nA New Approach to Accountancy; A Wonderful Companion; Volume I\nA New Approach to Accountancy Vol 1; Special Accounts\nA Teacher’s Wonderful Companion Vol. II; Company Accounts\nRole of internal Control Systems on Firm Performance of Companies Quoted in the Nairobi Securities Exchange\nforensic accounting, financial reporting quality; accountability practices and fraud prevention of devolved units Kenya."
  }
]